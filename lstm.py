# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UXXkCD4dG7oV-31k_cpnl6UgD4Ovr1sk
"""

#example based on https://keras.io/guides/working_with_rnns/
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import pandas as pd

#Building model
embedding_dim = 50
vocab_size = 20000
max_length = 200

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))
model.add(LSTM(units=150, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(units=3, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

from time import time

#from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer

categories = [
    "neutral",
    "negative",
    "positive",
]

data_train = pd.read_json("train.jsonl", lines=True)

# Load data
#dataset link https://www.kaggle.com/datasets/fhamborg/news-articles-sentiment
data_train = pd.read_json("train.jsonl", lines=True)
data_test = pd.read_json("test.jsonl", lines=True)
#data_train.head()
data_test.head()

x_train = data_train.sentence
y_train = data_train.polarity

x_test = data_test.sentence
y_test = data_test.polarity


vectorizer = TfidfVectorizer()
x_train= vectorizer.fit_transform(x_train)
x_test= vectorizer.transform(x_test)

from sklearn.svm import SVC
svc = SVC(random_state=1)

svc.fit(x_train, y_train)
predictions = svc.predict(x_test)

from sklearn.metrics import classification_report
classification_report(predictions, y_test)

#print(x_train[0]) #results of sklearn SVM model training, 63% accuracy

import xgboost
from sklearn.preprocessing import LabelEncoder

xgb_dt = xgboost.XGBClassifier(random_state=1)
encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_test = encoder.transform(y_test)
xgb_dt.fit(x_train, y_train)
## then make predictions on the test portion (predict the labels of the rows from the test portion of X)
predictions = xgb_dt.predict(x_test)
#print(predictions)

classification_report(predictions, y_test) #resulst of xgboost model training, 58% accuracy

print(len(data_train), "Training")
print(len(data_test), "Test")

# Preparing sentence and label for training
X_train = data_train['sentence']
y_train = data_train['polarity']

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_train = pad_sequences(X_train, maxlen=max_length)

y_train_categorical = to_categorical(y_train, num_classes=3)

v_size = len(tokenizer.word_index) + 1
print(v_size)

# Testing data prep
X_val = data_test['sentence']
y_val = data_test['polarity']

X_val = tokenizer.texts_to_sequences(X_val)
X_val = pad_sequences(X_val, maxlen=max_length)

# Convert labels to categorical (if not already in one-hot encoding)
y_val_categorical = to_categorical(y_val, num_classes=3)

# Train the model [this is the old version of the model]
model.fit(X_train, y_train_categorical, epochs=10, batch_size=64, validation_data=(X_val, y_val_categorical))

# Evaluate the model
loss, accuracy = model.evaluate(X_val, y_val_categorical)
print(f'Loss: {loss}, Accuracy: {accuracy * 100:.2f}%')

#check dataset for all polarity options to see if there is an unbalanced level of training data

df = pd.read_json('train.jsonl', lines=True)
rows_with_precision_minus_one = df[df['polarity'] == -1]
# Get the count of rows with precision -1
count_precision_minus_one = len(rows_with_precision_minus_one)
print(f"Number of rows with polarity -1: {count_precision_minus_one}")

rows_with_precision_minus_one = df[df['polarity'] == 0]
# Get the count of rows with precision 0
count_precision_minus_one = len(rows_with_precision_minus_one)
print(f"Number of rows with polarity 0: {count_precision_minus_one}")

rows_with_precision_minus_one = df[df['polarity'] == 1]
# Get the count of rows with precision 1
count_precision_minus_one = len(rows_with_precision_minus_one)
print(f"Number of rows with polarity 1: {count_precision_minus_one}")

"""Slight bias will resamlpe the dataset os they have more similar spread of polarity

"""

#resample data, oversampling
from sklearn.utils import resample
import pandas as pd

# Assuming your DataFrame is named 'df'
# Replace 'your_dataset.csv' with the actual file path or name
df = pd.read_json('train.jsonl', lines=True)

# Separate classes
df_class_minus_one = df[df['polarity'] == -1]
df_class_0 = df[df['polarity'] == 0]
df_class_1 = df[df['polarity'] == 1]

# Oversample minority classes (class -1 and class 1 in this case)
df_class_minus_one_oversampled = resample(df_class_minus_one, replace=True, n_samples=len(df_class_0), random_state=42)
df_class_1_oversampled = resample(df_class_1, replace=True, n_samples=len(df_class_0), random_state=42)

# Combine oversampled minority classes with majority class
df_oversampled = pd.concat([df_class_minus_one_oversampled, df_class_0, df_class_1_oversampled])

# Shuffle the dataframe to mix the classes
df_oversampled = df_oversampled.sample(frac=1, random_state=42)

# Display the counts after oversampling
print(df_oversampled['polarity'].value_counts())

#now retrain a new model and tokenise and split the data
max_length = 200

x_predict = df_oversampled['sentence']
y_label = df_oversampled['polarity']

tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_predict)
x_predict = tokenizer.texts_to_sequences(x_predict)
x_predict = pad_sequences(x_predict, maxlen=max_length)

#splitting over sampled data into a training and testing sets
from sklearn.model_selection import train_test_split

# Split the data into training and validation sets
x_predict, x_p, y_label, y_l = train_test_split(x_predict, y_label, test_size=0.2, random_state=42)

y_label_categorical = to_categorical(y_label, num_classes=3)
y_l_categorical = to_categorical(y_l, num_classes=3)

# Print the lengths of the training and validation sets
print(len(x_predict), "Training sequences")
print(len(x_p), "Validation sequences")

print(len(y_label), "Training sequences")
print(len(y_l), "Validation sequences")

#creating new model

embedding_dim = 50
vocab_size = 20000
max_length = 200

m1 = Sequential()
m1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))
m1.add(LSTM(units=150, dropout=0.3, recurrent_dropout=0.3))  # Adjust units and dropout
m1.add(Dense(units=3, activation='softmax'))



m1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
m1.summary()

# Train the model
m1.fit(x_predict, y_label_categorical, epochs=10, batch_size=64, validation_data=(x_p, y_l_categorical))

# Evaluate the model
loss, accuracy = m1.evaluate(x_p, y_l_categorical)
print(f'Loss: {loss}, Accuracy: {accuracy * 100:.2f}%')