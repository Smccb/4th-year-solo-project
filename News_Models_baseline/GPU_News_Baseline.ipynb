{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(\"../Datasets/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "dataset.head()\n",
    "\n",
    "# shuffled_df = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "half_df = dataset.head(len(dataset)//2)\n",
    "dataset = half_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_name_to_remove = 'article_link'\n",
    "dataset = dataset.drop(columns=[column_name_to_remove])\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['headline'], dataset['is_sarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and vectorize the training text data using Tokenizer and pad_sequences\n",
    "max_length = 100\n",
    "tokenizer = Tokenizer()   #lower=False\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "\n",
    "# Tokenize and vectorize the testing text data using the same Tokenizer\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          1865900   \n",
      "                                                                 \n",
      " cu_dnnlstm_2 (CuDNNLSTM)    (None, 150)               151200    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                9664      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,030,989\n",
      "Trainable params: 2,030,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Define the vocabulary size based on the actual number of unique words in the training data\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_length = 100\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "m1 = Sequential()\n",
    "m1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "m1.add(CuDNNLSTM(units=150))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "m1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "167/167 [==============================] - 3s 15ms/step - loss: 0.6911 - accuracy: 0.5471 - val_loss: 0.6900 - val_accuracy: 0.5425\n",
      "Epoch 2/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.6867 - accuracy: 0.5565 - val_loss: 0.6884 - val_accuracy: 0.5425\n",
      "Epoch 3/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.6840 - accuracy: 0.5565 - val_loss: 0.6875 - val_accuracy: 0.5425\n",
      "Epoch 4/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.6817 - accuracy: 0.5565 - val_loss: 0.6852 - val_accuracy: 0.5425\n",
      "Epoch 5/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.6782 - accuracy: 0.5565 - val_loss: 0.6814 - val_accuracy: 0.5425\n",
      "Epoch 6/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.6715 - accuracy: 0.5565 - val_loss: 0.6727 - val_accuracy: 0.5425\n",
      "Epoch 7/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.6561 - accuracy: 0.5604 - val_loss: 0.6543 - val_accuracy: 0.5492\n",
      "Epoch 8/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.6189 - accuracy: 0.6562 - val_loss: 0.6063 - val_accuracy: 0.7222\n",
      "Epoch 9/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.5621 - accuracy: 0.7603 - val_loss: 0.5581 - val_accuracy: 0.7664\n",
      "Epoch 10/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.4981 - accuracy: 0.8122 - val_loss: 0.5124 - val_accuracy: 0.7982\n",
      "Epoch 11/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.4461 - accuracy: 0.8388 - val_loss: 0.4872 - val_accuracy: 0.7593\n",
      "Epoch 12/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.4011 - accuracy: 0.8588 - val_loss: 0.4530 - val_accuracy: 0.8079\n",
      "Epoch 13/20\n",
      "167/167 [==============================] - 2s 13ms/step - loss: 0.3661 - accuracy: 0.8693 - val_loss: 0.4350 - val_accuracy: 0.8083\n",
      "Epoch 14/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.3368 - accuracy: 0.8773 - val_loss: 0.4230 - val_accuracy: 0.8136\n",
      "Epoch 15/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.3134 - accuracy: 0.8849 - val_loss: 0.4140 - val_accuracy: 0.8162\n",
      "Epoch 16/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.2912 - accuracy: 0.8951 - val_loss: 0.4119 - val_accuracy: 0.8173\n",
      "Epoch 17/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.2734 - accuracy: 0.9013 - val_loss: 0.4038 - val_accuracy: 0.8218\n",
      "Epoch 18/20\n",
      "167/167 [==============================] - 2s 13ms/step - loss: 0.2558 - accuracy: 0.9095 - val_loss: 0.4014 - val_accuracy: 0.8240\n",
      "Epoch 19/20\n",
      "167/167 [==============================] - 2s 13ms/step - loss: 0.2407 - accuracy: 0.9145 - val_loss: 0.4003 - val_accuracy: 0.8267\n",
      "Epoch 20/20\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.2264 - accuracy: 0.9214 - val_loss: 0.3976 - val_accuracy: 0.8248\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3976 - accuracy: 0.8248\n",
      "Loss: 0.3976247310638428, Accuracy: 82.48%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "m1.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = m1.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 7ms/step\n",
      "Precision: 0.8060\n",
      "Recall: 0.8126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred_prob_m1 = m1.predict(X_test)\n",
    "y_val_pred_m1 = (y_val_pred_prob_m1 > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "# Assuming y_test is in binary format (0 or 1)\n",
    "y_val_true_m1 = y_test\n",
    "\n",
    "# Calculate precision and recall for binary classification\n",
    "precision_m1 = precision_score(y_val_true_m1, y_val_pred_m1)\n",
    "recall_m1 = recall_score(y_val_true_m1, y_val_pred_m1)\n",
    "\n",
    "# print the results\n",
    "print(f'Precision: {precision_m1:.4f}')\n",
    "print(f'Recall: {recall_m1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
