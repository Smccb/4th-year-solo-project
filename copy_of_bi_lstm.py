# -*- coding: utf-8 -*-
"""Copy of BI-LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IxPnqe-ZWiZUYX97uVrLlzxPsJv3nr1R
"""

#example on keras documentation https://keras.io/examples/nlp/bidirectional_lstm_imdb/
#dataset https://www.kaggle.com/datasets/thedevastator/new-dataset-for-text-classification-ag-news
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

max_features = 20000  # Only consider the top 20k words
maxlen = 200  # Only consider the first 200 words of each news article review

#example on keras documentation https://keras.io/examples/nlp/bidirectional_lstm_imdb/
# Input for variable-length sequences of integers
inputs = keras.Input(shape=(None,), dtype="int32")
# Embed each integer in a 128-dimensional vector
x = layers.Embedding(max_features, 128)(inputs)
# Add 2 bidirectional LSTMs
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
x = layers.Bidirectional(layers.LSTM(64))(x)
# Add a classifier
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.summary()

#check number of uniqie words in dataset to update vocab size later
import pandas as pd
data = pd.read_csv("train.csv", nrows = 1000) #could also direct to kaggle website and not enter the name directly
data.head()
#data.info()

#budilding model
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# vocab size
max_features = 20000

# Input for variable-length sequences of integers
inputs = keras.Input(shape=(None,), dtype="int32")

# Embed each integer in a 128-dimensional vector
x = layers.Embedding(max_features, 128)(inputs)

# Add 2 bidirectional LSTMs
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
x = layers.Bidirectional(layers.LSTM(64))(x)

# Add a classifier with output shape matching the number of classes (4 in this case)
num_classes = 4  # Number of classes
outputs = layers.Dense(num_classes, activation="softmax")(x)

model = keras.Model(inputs, outputs)
model.summary()

#used https://www.kaggle.com/code/ybrenning/simple-feature-extractor-bert-model#Loading-the-Dataset and labs from data analysis for help
import pandas as pd
data = pd.read_csv("train.csv", nrows = 1000) #could also direct to kaggle website and not enter the name directly
data.head()
#df.shape

data.tail

data.shape

data.dtypes

#load data

from tensorflow.keras.preprocessing.text import Tokenizer

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
#getting text and label using pandas
texts = data['text']
labels = data['label']

#Tokenize the text data
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

#Padding sequences to a fixed length

YOUR_MAX_SEQUENCE_LENGTH =  200

maxlen = YOUR_MAX_SEQUENCE_LENGTH  # Replace with your desired sequence length
X = pad_sequences(sequences, maxlen=maxlen)

Y = labels  #label encoding

#Split your data into training and validation sets
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

print(len(x_train), "Training sequences")
print(len(x_val), "Validation sequences")

#Compile model with metrics
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

#training model on data
history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))



"""--------------------------------------------------------------------
Previous runs: 1)

Epoch 1/10
25/25 [==============================] - 21s 565ms/step - loss: 1.2865 - accuracy: 0.4550 - val_loss: 1.1901 - val_accuracy: 0.4850
Epoch 2/10
25/25 [==============================] - 12s 471ms/step - loss: 0.8921 - accuracy: 0.5850 - val_loss: 0.8553 - val_accuracy: 0.6250
Epoch 3/10
25/25 [==============================] - 11s 437ms/step - loss: 0.5476 - accuracy: 0.7563 - val_loss: 0.7089 - val_accuracy: 0.7000
Epoch 4/10
25/25 [==============================] - 11s 445ms/step - loss: 0.2477 - accuracy: 0.9425 - val_loss: 0.8596 - val_accuracy: 0.7250
Epoch 5/10
25/25 [==============================] - 12s 474ms/step - loss: 0.0840 - accuracy: 0.9862 - val_loss: 0.8561 - val_accuracy: 0.7350
Epoch 6/10
25/25 [==============================] - 12s 475ms/step - loss: 0.1394 - accuracy: 0.9588 - val_loss: 0.8876 - val_accuracy: 0.7200
Epoch 7/10
25/25 [==============================] - 12s 483ms/step - loss: 0.0805 - accuracy: 0.9787 - val_loss: 0.9850 - val_accuracy: 0.6750
Epoch 8/10
25/25 [==============================] - 12s 481ms/step - loss: 0.0553 - accuracy: 0.9850 - val_loss: 0.7133 - val_accuracy: 0.7800
Epoch 9/10
25/25 [==============================] - 12s 488ms/step - loss: 0.0385 - accuracy: 0.9900 - val_loss: 0.7550 - val_accuracy: 0.7750
Epoch 10/10
25/25 [==============================] - 12s 483ms/step - loss: 0.0336 - accuracy: 0.9887 - val_loss: 0.7245 - val_accuracy: 0.7700

2)

Epoch 1/10
25/25 [==============================] - 24s 688ms/step - loss: 1.2915 - accuracy: 0.4575 - val_loss: 1.2155 - val_accuracy: 0.4850
Epoch 2/10
25/25 [==============================] - 14s 572ms/step - loss: 0.9268 - accuracy: 0.5925 - val_loss: 0.9144 - val_accuracy: 0.6550
Epoch 3/10
25/25 [==============================] - 13s 498ms/step - loss: 0.4463 - accuracy: 0.8363 - val_loss: 0.8200 - val_accuracy: 0.7250
Epoch 4/10
25/25 [==============================] - 14s 564ms/step - loss: 0.1817 - accuracy: 0.9425 - val_loss: 0.9394 - val_accuracy: 0.7500
Epoch 5/10
25/25 [==============================] - 13s 520ms/step - loss: 0.0847 - accuracy: 0.9812 - val_loss: 1.2878 - val_accuracy: 0.7050
Epoch 6/10
25/25 [==============================] - 12s 474ms/step - loss: 0.0427 - accuracy: 0.9912 - val_loss: 1.2806 - val_accuracy: 0.7200
Epoch 7/10
25/25 [==============================] - 14s 557ms/step - loss: 0.0362 - accuracy: 0.9950 - val_loss: 1.3379 - val_accuracy: 0.7200
Epoch 8/10
25/25 [==============================] - 13s 523ms/step - loss: 0.0331 - accuracy: 0.9950 - val_loss: 1.3588 - val_accuracy: 0.7300
Epoch 9/10
25/25 [==============================] - 12s 468ms/step - loss: 0.0304 - accuracy: 0.9950 - val_loss: 1.4135 - val_accuracy: 0.7250
Epoch 10/10
25/25 [==============================] - 12s 495ms/step - loss: 0.0270 - accuracy: 0.9950 - val_loss: 1.4321 - val_accuracy: 0.7200
"""