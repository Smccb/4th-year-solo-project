{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_json(\"Datasets/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "twitter_df = pd.read_json(\"Datasets/data_without_hashtags.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()\n",
    "column_name_to_remove = 'article_link'\n",
    "news_df = news_df.drop(columns=[column_name_to_remove])\n",
    "\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isSarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@0430yes i hope youre lurking rn. i want to li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05 really taught me a valuable lesson I'm neve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@098BERRY Never had a voice to protest, so you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@0hMySt4rs Rest in peace &amp; love to you and you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100 days until Christmas! ðŸŒ² #too soon  ready yet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  isSarcastic\n",
       "0  @0430yes i hope youre lurking rn. i want to li...            0\n",
       "1  05 really taught me a valuable lesson I'm neve...            0\n",
       "2  @098BERRY Never had a voice to protest, so you...            0\n",
       "3  @0hMySt4rs Rest in peace & love to you and you...            0\n",
       "4   100 days until Christmas! ðŸŒ² #too soon  ready yet            0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isSarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  isSarcastic\n",
       "0  former versace store clerk sues over secret 'b...            0\n",
       "1  the 'roseanne' revival catches up to our thorn...            0\n",
       "2  mom starting to fear son's web series closest ...            1\n",
       "3  boehner just wants wife to listen, not come up...            1\n",
       "4  j.k. rowling wishes snape happy birthday in th...            0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = news_df.rename(columns={\"is_sarcastic\": \"isSarcastic\"})\n",
    "news_df = news_df.rename(columns={\"headline\": \"text\"})\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isSarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nervous as hell for no good reason.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manchester and Salford in top five of national...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sad how some people don't realize the good peo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow i just had the best dream ever, it neeeds ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yay school my favorite thing to do with my lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  isSarcastic\n",
       "0                Nervous as hell for no good reason.            0\n",
       "1  Manchester and Salford in top five of national...            0\n",
       "2  sad how some people don't realize the good peo...            0\n",
       "3  Wow i just had the best dream ever, it neeeds ...            0\n",
       "4  Yay school my favorite thing to do with my lif...            1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df = twitter_df.sample(frac=1).reset_index(drop=True)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([news_df, twitter_df], ignore_index=True)\n",
    "dataset = combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for each value in the 'isSarcastic' column:\n",
      "isSarcastic\n",
      "0    30212\n",
      "1    30212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#undersampling here\n",
    "\n",
    "# Count the number of instances in each class\n",
    "class_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Find the class with more items\n",
    "majority_class = class_counts.idxmax()\n",
    "\n",
    "# Find the class with fewer items\n",
    "minority_class = class_counts.idxmin()\n",
    "\n",
    "# Count the number of instances in the minority class\n",
    "minority_class_count = class_counts[minority_class]\n",
    "\n",
    "# Sample the majority class to match the number of instances in the minority class\n",
    "majority_class_sampled = dataset[dataset['isSarcastic'] == majority_class].sample(n=minority_class_count, random_state=42)\n",
    "\n",
    "# Concatenate the sampled majority class with the minority class\n",
    "balanced_data = pd.concat([majority_class_sampled, dataset[dataset['isSarcastic'] == minority_class]])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "dataset = balanced_data\n",
    "# Now, 'balanced_data' contains a balanced dataset where both classes have the same number of instances\n",
    "\n",
    "sarcastic_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of rows for each value in the 'isSarcastic' column:\")\n",
    "print(sarcastic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    # Define the regex pattern to match @ mentions followed by numbers\n",
    "    pattern = re.compile(r'@\\d+')\n",
    "\n",
    "    # Remove @ mentions using the pattern\n",
    "    return pattern.sub('person', text)\n",
    "\n",
    "# Load the DataFrame\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "\n",
    "# Apply remove_user_mentions function to the 'text' column\n",
    "dataset['text'] = dataset['text'].apply(remove_user_mentions)\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "dataset.to_json(\"updated_data_without_mentions.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "# Example mapping of abbreviations to their full forms\n",
    "abbreviation_mapping = {\n",
    "    'OMG': 'oh my god',\n",
    "    'DM': 'direct message',\n",
    "    'BTW': 'by the way',\n",
    "    'BRB': 'be right back',\n",
    "    'RT': 'retweet',\n",
    "    'FTW': 'for the win',\n",
    "    'QOTD': 'quote of the day',\n",
    "    'IDK': 'I do not know',\n",
    "    'ICYMI': 'in case you missed it',\n",
    "    'IRL': 'in real life',\n",
    "    'IMHO': 'in my humble opinion',\n",
    "    'IMO': 'I do not know',\n",
    "    'LOL': 'laugh out loud',\n",
    "    'LMAO': 'laugh my ass off',\n",
    "    'NTS': 'note to self',\n",
    "    'F2F': 'face to face',\n",
    "    'B4': 'before',\n",
    "    'DM': 'direct message',\n",
    "    'CC': 'carbon copy',\n",
    "    'SMH': 'shaking my head',\n",
    "    'STFU': 'shut the fuck up',\n",
    "    'BFN': 'by for now',\n",
    "    'AFAIK': 'as far as I know',\n",
    "    'TY': 'thank you',\n",
    "    'YW': 'you are welcome',\n",
    "    'THX': 'thanks',\n",
    "    'TIL': 'today I learned',\n",
    "    'AMA': 'ask me anything',\n",
    "    'JK': 'just kidding',\n",
    "    'NSFW': 'Not Safe for Work',\n",
    "    'OOTD': 'outfit of the day',\n",
    "    'TLDR': 'too long did not read',\n",
    "    'TL;DR': 'too long; did not read',\n",
    "    'GIF': 'graphics interchange format'\n",
    "}\n",
    "\n",
    "# Function to replace abbreviations\n",
    "def replace_abbreviations(text):\n",
    "    tokens = text.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.upper() in abbreviation_mapping:\n",
    "            tokens[i] = abbreviation_mapping[token.upper()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply functions to remove hashtags and replace abbreviations to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(lambda x: x.upper())  # Convert text to uppercase\n",
    "dataset['text'] = dataset['text'].apply(replace_abbreviations)\n",
    "\n",
    "# Restore original capitalization\n",
    "original_capitalization = lambda x: ''.join([a if b.islower() else a.lower() for a, b in zip(x, dataset['text'][0])])\n",
    "dataset['text'] = dataset['text'].apply(original_capitalization)\n",
    "\n",
    "# Save the updated DataFrame to a JSON file\n",
    "dataset.to_json('abbreviations_removed.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  isSarcastic\n",
      "0      former versace store clerk sues over secret 'b...            0\n",
      "1      the 'roseanne' revival catches up to our thorn...            0\n",
      "2      mom starting to fear son's web series closest ...            1\n",
      "3      boehner just wants wife to listen, not come up...            1\n",
      "4      j.k. rowling wishes snape happy birthday in th...            0\n",
      "...                                                  ...          ...\n",
      "66484  @rioferdy5 personmag @paulpogba waste of money...            0\n",
      "66485  technically i agree. they usually don't get ca...            1\n",
      "66486  @theweeknd i miss you. come over and sing to m...            0\n",
      "66487  but they hardly get proper controversial on th...            0\n",
      "66488  wearing an outfit you like can make a day 10x ...            1\n",
      "\n",
      "[66489 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import json\n",
    "\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "df = dataset\n",
    "\n",
    "# Function to replace emojis with words\n",
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))  # Ensure emojis are separated by spaces\n",
    "\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    emoticon_dict = {\n",
    "    ':)': 'smile',\n",
    "    ':(': 'frown',\n",
    "    ':D': 'big smile',\n",
    "    ':P': 'tongue out',\n",
    "    ';)': 'wink',\n",
    "    ':O': 'surprise',\n",
    "    ':|': 'neutral',\n",
    "    ':/': 'uncertain',\n",
    "    \":'(\": 'tears of sadness',\n",
    "    \":'D\": 'tears of joy',\n",
    "    ':*': 'kiss',\n",
    "    ':@': 'angry',\n",
    "    ':x': 'mouth shut',\n",
    "    ':3': 'cute',\n",
    "    ':$': 'embarrassed',\n",
    "    \":')\": 'single tear',\n",
    "    ':p': 'tongue out'\n",
    "}\n",
    "\n",
    "\n",
    "    # #Construct regex pattern using re.escape() to escape special characters\n",
    "    # pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # # Replace emoticons using the pattern\n",
    "    # return pattern.sub(lambda match: emoticon_dict.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "    # Convert emoticon keys to lowercase\n",
    "    emoticon_dict_lower = {key.lower(): value for key, value in emoticon_dict.items()}\n",
    "\n",
    "    # Construct regex pattern using re.escape() to escape special characters\n",
    "    pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict_lower.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # Replace emoticons using the pattern\n",
    "    return pattern.sub(lambda match: emoticon_dict_lower.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply functions to replace emojis and emoticons and update DataFrame columns\n",
    "df['text'] = df['text'].apply(replace_emojis)\n",
    "df['text'] = df['text'].apply(replace_emoticons)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "data_dict = df.to_dict()\n",
    "\n",
    "with open('removedEmoji.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['isSarcastic'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Tokenize and vectorize the training text data using Tokenizer and pad_sequences\n",
    "max_length = 140\n",
    "tokenizer = Tokenizer()   #lower=False\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "\n",
    "# Tokenize and vectorize the testing text data using the same Tokenizer\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (53191, 140)\n",
      "Shape of X_test: (13298, 140)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 140\n",
    "\n",
    "# Initialize TweetTokenizer\n",
    "tweetTokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize training text data\n",
    "X_train_tokenized = [tweetTokenizer.tokenize(text) for text in X_train]\n",
    "\n",
    "# Tokenize testing text data\n",
    "X_test_tokenized = [tweetTokenizer.tokenize(text) for text in X_test]\n",
    "\n",
    "# Create Tokenizer instance\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer on training text data\n",
    "tokenizer.fit_on_texts(X_train_tokenized)\n",
    "\n",
    "# Convert text data to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_tokenized)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_tokenized)\n",
    "\n",
    "# Pad sequences\n",
    "X_train = pad_sequences(X_train_sequences, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test_sequences, maxlen=max_length)\n",
    "\n",
    "# Display shapes of resulting matrices\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 140, 100)          4527300   \n",
      "                                                                 \n",
      " cu_dnnlstm_5 (CuDNNLSTM)    (None, 150)               151200    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                9664      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,692,389\n",
      "Trainable params: 4,692,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Define the vocabulary size based on the actual number of unique words in the training data\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_length = 140\n",
    "optimizer = Adam(learning_rate=0.000009)\n",
    "m1 = Sequential()\n",
    "m1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "m1.add(CuDNNLSTM(units=150))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "m1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "832/832 [==============================] - 21s 24ms/step - loss: 0.6887 - accuracy: 0.5468 - val_loss: 0.6892 - val_accuracy: 0.5372\n",
      "Epoch 2/20\n",
      "832/832 [==============================] - 20s 24ms/step - loss: 0.6852 - accuracy: 0.5477 - val_loss: 0.6857 - val_accuracy: 0.5372\n",
      "Epoch 3/20\n",
      "832/832 [==============================] - 20s 24ms/step - loss: 0.6754 - accuracy: 0.5578 - val_loss: 0.6647 - val_accuracy: 0.5720\n",
      "Epoch 4/20\n",
      "832/832 [==============================] - 20s 24ms/step - loss: 0.6189 - accuracy: 0.6671 - val_loss: 0.5980 - val_accuracy: 0.6866\n",
      "Epoch 5/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.5583 - accuracy: 0.7203 - val_loss: 0.5664 - val_accuracy: 0.7139\n",
      "Epoch 6/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.5181 - accuracy: 0.7523 - val_loss: 0.5449 - val_accuracy: 0.7259\n",
      "Epoch 7/20\n",
      "832/832 [==============================] - 20s 24ms/step - loss: 0.4846 - accuracy: 0.7736 - val_loss: 0.5276 - val_accuracy: 0.7428\n",
      "Epoch 8/20\n",
      "832/832 [==============================] - 20s 24ms/step - loss: 0.4564 - accuracy: 0.7901 - val_loss: 0.5172 - val_accuracy: 0.7500\n",
      "Epoch 9/20\n",
      "832/832 [==============================] - 20s 24ms/step - loss: 0.4319 - accuracy: 0.8042 - val_loss: 0.5125 - val_accuracy: 0.7559\n",
      "Epoch 10/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.4103 - accuracy: 0.8168 - val_loss: 0.5079 - val_accuracy: 0.7619\n",
      "Epoch 11/20\n",
      "832/832 [==============================] - 20s 24ms/step - loss: 0.3912 - accuracy: 0.8270 - val_loss: 0.5092 - val_accuracy: 0.7638\n",
      "Epoch 12/20\n",
      "832/832 [==============================] - 20s 24ms/step - loss: 0.3734 - accuracy: 0.8368 - val_loss: 0.5055 - val_accuracy: 0.7676\n",
      "Epoch 13/20\n",
      "832/832 [==============================] - 20s 24ms/step - loss: 0.3574 - accuracy: 0.8448 - val_loss: 0.5114 - val_accuracy: 0.7684\n",
      "Epoch 14/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.3421 - accuracy: 0.8529 - val_loss: 0.5118 - val_accuracy: 0.7646\n",
      "Epoch 15/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.3282 - accuracy: 0.8598 - val_loss: 0.5170 - val_accuracy: 0.7661\n",
      "Epoch 16/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.3146 - accuracy: 0.8667 - val_loss: 0.5230 - val_accuracy: 0.7663\n",
      "Epoch 17/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.3021 - accuracy: 0.8723 - val_loss: 0.5304 - val_accuracy: 0.7662\n",
      "Epoch 18/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.2900 - accuracy: 0.8779 - val_loss: 0.5357 - val_accuracy: 0.7650\n",
      "Epoch 19/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.2787 - accuracy: 0.8842 - val_loss: 0.5537 - val_accuracy: 0.7644\n",
      "Epoch 20/20\n",
      "832/832 [==============================] - 19s 23ms/step - loss: 0.2680 - accuracy: 0.8898 - val_loss: 0.5574 - val_accuracy: 0.7639\n",
      "416/416 [==============================] - 2s 5ms/step - loss: 0.5574 - accuracy: 0.7639\n",
      "Loss: 0.5574001669883728, Accuracy: 76.39%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "m1.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = m1.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416/416 [==============================] - 2s 4ms/step\n",
      "Precision: 0.7654\n",
      "Recall: 0.7065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred_prob_m1 = m1.predict(X_test)\n",
    "y_val_pred_m1 = (y_val_pred_prob_m1 > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "# Assuming y_test is in binary format (0 or 1)\n",
    "y_val_true_m1 = y_test\n",
    "\n",
    "# Calculate precision and recall for binary classification\n",
    "precision_m1 = precision_score(y_val_true_m1, y_val_pred_m1)\n",
    "recall_m1 = recall_score(y_val_true_m1, y_val_pred_m1)\n",
    "\n",
    "# print the results\n",
    "print(f'Precision: {precision_m1:.4f}')\n",
    "print(f'Recall: {recall_m1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
