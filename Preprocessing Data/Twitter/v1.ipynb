{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for each value in the 'isSarcastic' column:\n",
      "isSarcastic\n",
      "0    21292\n",
      "1    18488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Assuming \"is_sarcastic\" is the column you're interested in\n",
    "sarcastic_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of rows for each value in the 'isSarcastic' column:\")\n",
    "print(sarcastic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for each value in the 'isSarcastic' column:\n",
      "isSarcastic\n",
      "0    18488\n",
      "1    18488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#update number of rows in dataset undersampling\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Count the number of instances in each class\n",
    "class_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Find the class with more items\n",
    "majority_class = class_counts.idxmax()\n",
    "\n",
    "# Find the class with fewer items\n",
    "minority_class = class_counts.idxmin()\n",
    "\n",
    "# Count the number of instances in the minority class\n",
    "minority_class_count = class_counts[minority_class]\n",
    "\n",
    "# Sample the majority class to match the number of instances in the minority class\n",
    "majority_class_sampled = dataset[dataset['isSarcastic'] == majority_class].sample(n=minority_class_count, random_state=42)\n",
    "\n",
    "# Concatenate the sampled majority class with the minority class\n",
    "balanced_data = pd.concat([majority_class_sampled, dataset[dataset['isSarcastic'] == minority_class]])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "dataset = balanced_data\n",
    "# Now, 'balanced_data' contains a balanced dataset where both classes have the same number of instances\n",
    "\n",
    "sarcastic_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of rows for each value in the 'isSarcastic' column:\")\n",
    "print(sarcastic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#half the dataset and undersample\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'data' with columns 'is_sarcastic' and other features\n",
    "\n",
    "# Count the number of instances in each class\n",
    "class_counts = data['isSarcastic'].value_counts()\n",
    "\n",
    "# Find the class with more items\n",
    "majority_class = class_counts.idxmax()\n",
    "\n",
    "# Find the class with fewer items\n",
    "minority_class = class_counts.idxmin()\n",
    "\n",
    "# Count the number of instances in the minority class\n",
    "minority_class_count = class_counts[minority_class]\n",
    "\n",
    "# Sample the majority class to match the number of instances in the minority class\n",
    "majority_class_sampled = data[data['isSarcastic'] == majority_class].sample(n=minority_class_count, random_state=42)\n",
    "\n",
    "# Concatenate the sampled majority class with the minority class\n",
    "balanced_data = pd.concat([majority_class_sampled, data[data['isSarcastic'] == minority_class]])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the balanced dataset into two equal parts\n",
    "half_index = len(balanced_data) // 2\n",
    "balanced_data_part1 = balanced_data.iloc[:half_index]\n",
    "balanced_data_part2 = balanced_data.iloc[half_index:]\n",
    "\n",
    "# Now, 'balanced_data_part1' and 'balanced_data_part2' contain two equal parts of the balanced dataset\n",
    "dataset = balanced_data_part1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isSarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I hate  two -faced  people. It's so hard to de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What a day for Cyrus Jones!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter has a bird as its logo, that's why whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@arden_cho How does it feel to know that peopl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@BlackIrishI @Dolly0811 @akawhit1 @sheila14all...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  isSarcastic\n",
       "0  I hate  two -faced  people. It's so hard to de...            0\n",
       "1                        What a day for Cyrus Jones!            0\n",
       "2  Twitter has a bird as its logo, that's why whe...            1\n",
       "3  @arden_cho How does it feel to know that peopl...            0\n",
       "4  @BlackIrishI @Dolly0811 @akawhit1 @sheila14all...            0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "\n",
    "#Preprocessing starts here\n",
    "\n",
    "\n",
    "#stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized texts:\n",
      "0        I hate  two -faced  people. It's so hard to de...\n",
      "1                              What a day for Cyrus Jones!\n",
      "2        Twitter has a bird as its logo, that's why whe...\n",
      "3        @arden_cho How does it feel to know that peopl...\n",
      "4        @BlackIrishI @Dolly0811 @akawhit1 @sheila14all...\n",
      "                               ...                        \n",
      "18483    many happy returnS Steven, sending best wishes...\n",
      "18484        just the pitter-patter of rainfall in my mind\n",
      "18485    @italiaricci @scottmfoster in having Chasing L...\n",
      "18486    @amyschumer do yourself a favor and delete you...\n",
      "18487    Look I'm sorry about your lamp but automatic k...\n",
      "Name: text, Length: 18488, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply remove_stopwords function to the 'text' column\n",
    "df['text_without_stopwords'] = df['text'].apply(remove_stopwords)\n",
    "\n",
    "print(\"Tokenized texts:\")\n",
    "print(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################replace emojis and emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 0.0/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 71.7/433.8 kB 653.6 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 143.4/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 143.4/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 276.5/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 368.6/433.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 433.8/433.8 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  isSarcastic  \\\n",
      "0      I hate  two -faced  people. It's so hard to de...            0   \n",
      "1                            What a day for Cyrus Jones!            0   \n",
      "2      Twitter has a bird as its logo, that's why whe...            1   \n",
      "3      @arden_cho How does it feel to know that peopl...            0   \n",
      "4      @BlackIrishI @Dolly0811 @akawhit1 @sheila14all...            0   \n",
      "...                                                  ...          ...   \n",
      "18483  many happy returnS Steven, sending best wishes...            0   \n",
      "18484      just the pitter-patter of rainfall in my mind            0   \n",
      "18485  @italiaricci @scottmfoster in having Chasing L...            0   \n",
      "18486  @amyschumer do yourself a favor and delete you...            0   \n",
      "18487  Look I'm sorry about your lamp but automatic k...            1   \n",
      "\n",
      "                                  text_without_stopwords  \n",
      "0      hate two -faced people . 's hard decide face s...  \n",
      "1                                      day Cyrus Jones !  \n",
      "2      Twitter bird logo , 's join 're egg . home but...  \n",
      "3      @ arden_cho feel know people watching episodes...  \n",
      "4      @ BlackIrishI @ Dolly0811 @ akawhit1 @ sheila1...  \n",
      "...                                                  ...  \n",
      "18483  many happy returnS Steven , sending best wishe...  \n",
      "18484                        pitter-patter rainfall mind  \n",
      "18485  @ italiaricci @ scottmfoster Chasing Life with...  \n",
      "18486  @ amyschumer favor delete account . Sew mouth ...  \n",
      "18487  Look 'm sorry lamp automatic karate serious di...  \n",
      "\n",
      "[18488 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import json\n",
    "\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "df = dataset\n",
    "\n",
    "# Function to replace emojis with words\n",
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))  # Ensure emojis are separated by spaces\n",
    "\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    emoticon_dict = {\n",
    "    ':)': 'smile',\n",
    "    ':(': 'frown',\n",
    "    ':D': 'big smile',\n",
    "    ':P': 'tongue out',\n",
    "    ';)': 'wink',\n",
    "    ':O': 'surprise',\n",
    "    ':|': 'neutral',\n",
    "    ':/': 'uncertain',\n",
    "    \":'(\": 'tears of sadness',\n",
    "    \":'D\": 'tears of joy',\n",
    "    ':*': 'kiss',\n",
    "    ':@': 'angry',\n",
    "    ':x': 'mouth shut',\n",
    "    ':3': 'cute',\n",
    "    ':$': 'embarrassed',\n",
    "    \":')\": 'single tear',\n",
    "    ':p': 'tongue out'\n",
    "}\n",
    "\n",
    "\n",
    "    # #Construct regex pattern using re.escape() to escape special characters\n",
    "    # pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # # Replace emoticons using the pattern\n",
    "    # return pattern.sub(lambda match: emoticon_dict.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "    # Convert emoticon keys to lowercase\n",
    "    emoticon_dict_lower = {key.lower(): value for key, value in emoticon_dict.items()}\n",
    "\n",
    "    # Construct regex pattern using re.escape() to escape special characters\n",
    "    pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict_lower.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # Replace emoticons using the pattern\n",
    "    return pattern.sub(lambda match: emoticon_dict_lower.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply functions to replace emojis and emoticons and update DataFrame columns\n",
    "df['text'] = df['text'].apply(replace_emojis)\n",
    "df['text'] = df['text'].apply(replace_emoticons)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "data_dict = df.to_dict()\n",
    "\n",
    "with open('removedEmoji.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################# remove @s of users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removes @s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    # Define the regex pattern to match @ mentions followed by numbers\n",
    "    pattern = re.compile(r'@\\d+')\n",
    "\n",
    "    # Remove @ mentions using the pattern\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Load the DataFrame\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "df = dataset\n",
    "\n",
    "# Apply remove_user_mentions function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_user_mentions)\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "df.to_json(\"updated_data_without_mentions.json\", orient='records', lines=True)\n",
    "\n",
    "dataset = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removes @ and then replaces it with word person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    # Define the regex pattern to match @ mentions followed by numbers\n",
    "    pattern = re.compile(r'@\\d+')\n",
    "\n",
    "    # Remove @ mentions using the pattern\n",
    "    return pattern.sub('person', text)\n",
    "\n",
    "# Load the DataFrame\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "df = dataset\n",
    "\n",
    "# Apply remove_user_mentions function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_user_mentions)\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "df.to_json(\"updated_data_without_mentions.json\", orient='records', lines=True)\n",
    "\n",
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#spellcheck words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellcheckerNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB 653.6 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.2/6.8 MB 1.7 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/6.8 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.5/6.8 MB 3.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/6.8 MB 2.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.2/6.8 MB 3.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.4/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.8 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.0/6.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.1/6.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.2/6.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.3/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.4/6.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.6/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.7/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.9/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.9/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.2/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.2/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.3/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.5/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.6/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.1/6.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.1/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.2/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.2/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.3/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.9/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.2/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.5/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.1/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.4/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Split the indices of the DataFrame into two halves with stratified sampling\n",
    "indices_half1, indices_half2 = train_test_split(dataset.index, test_size=0.5, stratify=dataset['isSarcastic'], random_state=42)\n",
    "\n",
    "# Create DataFrame quarters using the selected indices\n",
    "dataset_half = dataset.loc[indices_half1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  isSarcastic\n",
      "13906  Now serving up great coffee from @topecacoffee...            0\n",
      "27947  I'm assuming Greek yogurt is just regular yogu...            1\n",
      "16807  Spread love In this Evening with Manwa Laage.....            0\n",
      "34788  RT The only benefit of dating a Srilankan girl...            1\n",
      "22679  @BudGirl555 awesome. I cannot wait. Overdue. P...            1\n",
      "...                                                  ...          ...\n",
      "36254  Thank God I still have 5 days to achieve my go...            1\n",
      "38668  when you know you typed ur password wrong but ...            1\n",
      "22865  Can't wait to celebrate my birthday in boot ca...            1\n",
      "17210  @tedcruz say a prayer cause no matter who is e...            0\n",
      "734            And I thought I was really sarcastic but.            0\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Sample 150 rows from the dataset\n",
    "sampled_dataset = dataset.sample(n=150, random_state=42)\n",
    "\n",
    "# Display the sampled dataset\n",
    "print(sampled_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "indices_half1, indices_half2 = train_test_split(dataset.index, test_size=0.5, stratify=dataset['isSarcastic'], random_state=42)\n",
    "\n",
    "# Split each half into two quarters\n",
    "indices_quarter1, indices_quarter2 = train_test_split(indices_half1, test_size=0.5, stratify=dataset.loc[indices_half1]['isSarcastic'], random_state=42)\n",
    "indices_quarter3, indices_quarter4 = train_test_split(indices_half2, test_size=0.5, stratify=dataset.loc[indices_half2]['isSarcastic'], random_state=42)\n",
    "\n",
    "# Create DataFrame quarters using the selected indices\n",
    "dataset_quarter1 = dataset.loc[indices_quarter1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sampled_dataset['text'], sampled_dataset['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_half['text'], dataset_half['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = dataset_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m             corrected_text\u001b[38;5;241m.\u001b[39mappend(word)  \u001b[38;5;66;03m# Keep the original word if no correction is found\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected_text)\n\u001b[1;32m---> 19\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect_spelling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#corrected_text = correct_spelling(test_text)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#print(corrected_text)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Convert DataFrame to JSON and write to a file\u001b[39;00m\n\u001b[0;32m     25\u001b[0m dataset\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4791\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[88], line 11\u001b[0m, in \u001b[0;36mcorrect_spelling\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m---> 11\u001b[0m     corrected_word \u001b[38;5;241m=\u001b[39m \u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m corrected_word:\n\u001b[0;32m     13\u001b[0m         corrected_text\u001b[38;5;241m.\u001b[39mappend(corrected_word)\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:158\u001b[0m, in \u001b[0;36mSpellChecker.correction\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The most probable correct spelling for the word\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    word (str): The word to correct\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    str: The most likely candidate or None if no correction is present\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m word \u001b[38;5;241m=\u001b[39m ensure_unicode(word)\n\u001b[1;32m--> 158\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:185\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 185\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:252\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medit_distance_1(e1))]\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import pandas as pd\n",
    "\n",
    "test_text = \"This is a test #hdsjs\"\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spelling(text):\n",
    "    corrected_text = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word:\n",
    "            corrected_text.append(corrected_word)\n",
    "        else:\n",
    "            corrected_text.append(word)  # Keep the original word if no correction is found\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(correct_spelling)\n",
    "\n",
    "#corrected_text = correct_spelling(test_text)\n",
    "#print(corrected_text)\n",
    "\n",
    "# Convert DataFrame to JSON and write to a file\n",
    "dataset.to_json('processed_data.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#abreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Example mapping of abbreviations to their full forms\n",
    "abbreviation_mapping = {\n",
    "    'OMG': 'oh my god',\n",
    "    'DM': 'direct message',\n",
    "    'BTW': 'by the way',\n",
    "    'BRB': 'be right back',\n",
    "    'RT': 'retweet',\n",
    "    'FTW': 'for the win',\n",
    "    'QOTD': 'quote of the day',\n",
    "    'IDK': 'I do not know',\n",
    "    'ICYMI': 'in case you missed it',\n",
    "    'IRL': 'in real life',\n",
    "    'IMHO': 'in my humble opinion',\n",
    "    'IMO': 'I do not know',\n",
    "    'LOL': 'laugh out loud',\n",
    "    'LMAO': 'laughing my ass off',\n",
    "    'LMFAO': 'laughing my fucking ass off',\n",
    "    'NTS': 'note to self',\n",
    "    'F2F': 'face to face',\n",
    "    'B4': 'before',\n",
    "    'DM': 'direct message',\n",
    "    'CC': 'carbon copy',\n",
    "    'SMH': 'shaking my head',\n",
    "    'STFU': 'shut the fuck up',\n",
    "    'BFN': 'by for now',\n",
    "    'AFAIK': 'as far as I know',\n",
    "    'TY': 'thank you',\n",
    "    'YW': 'you are welcome',\n",
    "    'THX': 'thanks'\n",
    "}\n",
    "\n",
    "# Function to replace abbreviations\n",
    "def replace_abbreviations(text):\n",
    "    tokens = text.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.upper() in abbreviation_mapping:\n",
    "            tokens[i] = abbreviation_mapping[token.upper()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply functions to remove hashtags and replace abbreviations to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(lambda x: x.upper())  # Convert text to uppercase\n",
    "dataset['text'] = dataset['text'].apply(replace_abbreviations)\n",
    "\n",
    "# Restore original capitalization\n",
    "original_capitalization = lambda x: ''.join([a if b.islower() else a.lower() for a, b in zip(x, dataset['text'][0])])\n",
    "dataset['text'] = dataset['text'].apply(original_capitalization)\n",
    "\n",
    "# Save the updated DataFrame to a JSON file\n",
    "dataset.to_json('abbreviations_removed.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#remove all hashtags and don't replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Example DataFrame with 'text' column containing Twitter data\n",
    "#dataset = pd.DataFrame({'text': [\"This is a tweet with #hashtags\", \"Another tweet with #morehashtags\", \"Yet another tweet with #hashtags\"]})\n",
    "\n",
    "# Function to remove hashtags from a single text\n",
    "def remove_hashtags(text):\n",
    "    pattern = r'\\#\\w+'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# Apply the function to remove hashtags to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(remove_hashtags)\n",
    "\n",
    "\n",
    "data_dict = dataset.to_dict()\n",
    "\n",
    "with open('all#Removed.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "#unicode escapes fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_unicode(text):\n",
    "    # Decode Unicode escape sequences using 'unicode_escape' codec\n",
    "    decoded_text = text.encode().decode('unicode_escape')\n",
    "    # Encode the resulting Unicode string using 'latin-1' codec, ignoring errors\n",
    "    encoded_text = decoded_text.encode('latin-1', 'ignore')\n",
    "    return encoded_text.decode('utf-8')\n",
    "\n",
    "\n",
    "# Apply the function to each element in the dataset\n",
    "fixed_dataset = [decode_unicode(text) for text in dataset]\n",
    "\n",
    "with open('unicodeFixes.json', 'w') as f:\n",
    "    json.dump(fixed_dataset, f, indent=4)\n",
    "\n",
    "dataset = fixed_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read the dataset\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#dataset = pd.read_json(\"data_without_hashtags.json\")\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mdataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misSarcastic\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Tokenize and vectorize the training text data using Tokenizer and pad_sequences\u001b[39;00m\n\u001b[0;32m     14\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m140\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Read the dataset\n",
    "#dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and vectorize the training text data using Tokenizer and pad_sequences\n",
    "max_length = 140\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=2)\n",
    "\n",
    "# Display the shapes of the resulting matrices\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "\n",
    "# Tokenize and vectorize the testing text data using the same Tokenizer\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "# Display the shape of X_test\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Define the vocabulary size based on the actual number of unique words in the training data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     12\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m140\u001b[39m\n\u001b[0;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000009\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Define the vocabulary size based on the actual number of unique words in the training data\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_length = 140\n",
    "\n",
    "optimizer = Adam(learning_rate=0.000009)\n",
    "m1 = Sequential()\n",
    "m1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "m1.add(CuDNNLSTM(units=150))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "m1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "232/232 [==============================] - 5s 17ms/step - loss: 0.6930 - accuracy: 0.5175 - val_loss: 0.6926 - val_accuracy: 0.5446\n",
      "Epoch 2/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.6924 - accuracy: 0.5520 - val_loss: 0.6918 - val_accuracy: 0.5706\n",
      "Epoch 3/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.6914 - accuracy: 0.5746 - val_loss: 0.6903 - val_accuracy: 0.5825\n",
      "Epoch 4/20\n",
      "232/232 [==============================] - 4s 15ms/step - loss: 0.6896 - accuracy: 0.5908 - val_loss: 0.6873 - val_accuracy: 0.5979\n",
      "Epoch 5/20\n",
      "232/232 [==============================] - 4s 15ms/step - loss: 0.6860 - accuracy: 0.6033 - val_loss: 0.6822 - val_accuracy: 0.6052\n",
      "Epoch 6/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.6800 - accuracy: 0.6091 - val_loss: 0.6753 - val_accuracy: 0.6063\n",
      "Epoch 7/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.6712 - accuracy: 0.6176 - val_loss: 0.6661 - val_accuracy: 0.6190\n",
      "Epoch 8/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.6600 - accuracy: 0.6329 - val_loss: 0.6574 - val_accuracy: 0.6255\n",
      "Epoch 9/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.6459 - accuracy: 0.6462 - val_loss: 0.6461 - val_accuracy: 0.6414\n",
      "Epoch 10/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.6256 - accuracy: 0.6705 - val_loss: 0.6315 - val_accuracy: 0.6504\n",
      "Epoch 11/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.5989 - accuracy: 0.6932 - val_loss: 0.6150 - val_accuracy: 0.6690\n",
      "Epoch 12/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.5690 - accuracy: 0.7135 - val_loss: 0.6043 - val_accuracy: 0.6763\n",
      "Epoch 13/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.5397 - accuracy: 0.7347 - val_loss: 0.5935 - val_accuracy: 0.6863\n",
      "Epoch 14/20\n",
      "232/232 [==============================] - 4s 15ms/step - loss: 0.5123 - accuracy: 0.7515 - val_loss: 0.5837 - val_accuracy: 0.6882\n",
      "Epoch 15/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.4893 - accuracy: 0.7675 - val_loss: 0.5808 - val_accuracy: 0.6958\n",
      "Epoch 16/20\n",
      "232/232 [==============================] - 4s 17ms/step - loss: 0.4715 - accuracy: 0.7801 - val_loss: 0.5791 - val_accuracy: 0.6982\n",
      "Epoch 17/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.4561 - accuracy: 0.7896 - val_loss: 0.5785 - val_accuracy: 0.6985\n",
      "Epoch 18/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.4406 - accuracy: 0.7988 - val_loss: 0.5790 - val_accuracy: 0.7025\n",
      "Epoch 19/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.4270 - accuracy: 0.8085 - val_loss: 0.5801 - val_accuracy: 0.7052\n",
      "Epoch 20/20\n",
      "232/232 [==============================] - 4s 16ms/step - loss: 0.4139 - accuracy: 0.8150 - val_loss: 0.5829 - val_accuracy: 0.7077\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.5829 - accuracy: 0.7077\n",
      "Loss: 0.5828745365142822, Accuracy: 70.77%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "m1.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = m1.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 1s 6ms/step\n",
      "Precision: 0.7173\n",
      "Recall: 0.6654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred_prob_m1 = m1.predict(X_test)\n",
    "y_val_pred_m1 = (y_val_pred_prob_m1 > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "# Assuming y_test is in binary format (0 or 1)\n",
    "y_val_true_m1 = y_test\n",
    "\n",
    "# Calculate precision and recall for binary classification\n",
    "precision_m1 = precision_score(y_val_true_m1, y_val_pred_m1)\n",
    "recall_m1 = recall_score(y_val_true_m1, y_val_pred_m1)\n",
    "\n",
    "# print the results\n",
    "print(f'Precision: {precision_m1:.4f}')\n",
    "print(f'Recall: {recall_m1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
