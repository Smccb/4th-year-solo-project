{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for each value in the 'isSarcastic' column:\n",
      "isSarcastic\n",
      "0    21292\n",
      "1    18488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Assuming \"is_sarcastic\" is the column you're interested in\n",
    "sarcastic_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of rows for each value in the 'isSarcastic' column:\")\n",
    "print(sarcastic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isSarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@0430yes i hope youre lurking rn. i want to li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05 really taught me a valuable lesson I'm neve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@098BERRY Never had a voice to protest, so you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@0hMySt4rs Rest in peace &amp; love to you and you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100 days until Christmas! ðŸŒ² #too soon  ready yet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  isSarcastic\n",
       "0  @0430yes i hope youre lurking rn. i want to li...            0\n",
       "1  05 really taught me a valuable lesson I'm neve...            0\n",
       "2  @098BERRY Never had a voice to protest, so you...            0\n",
       "3  @0hMySt4rs Rest in peace & love to you and you...            0\n",
       "4   100 days until Christmas! ðŸŒ² #too soon  ready yet            0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "\n",
    "#Preprocessing starts here\n",
    "\n",
    "\n",
    "#stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized texts:\n",
      "0        @0430yes i hope youre lurking rn. i want to li...\n",
      "1        05 really taught me a valuable lesson I'm neve...\n",
      "2        @098BERRY Never had a voice to protest, so you...\n",
      "3        @0hMySt4rs Rest in peace & love to you and you...\n",
      "4         100 days until Christmas! ðŸŒ² #too soon  ready yet\n",
      "                               ...                        \n",
      "39775    @Zendaya I could see the makeup artists giving...\n",
      "39776    @ZiggiWatkins11 Slvr... That's great name #NOT...\n",
      "39777    @zoso4986 @Nero He is the fag we need but not ...\n",
      "39778    Zuma sounding like Kanye West right now trying...\n",
      "39779    @ZZUCRU @UWDawgPack So true. Students - stick ...\n",
      "Name: text, Length: 39780, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply remove_stopwords function to the 'text' column\n",
    "df['text_without_stopwords'] = df['text'].apply(remove_stopwords)\n",
    "\n",
    "print(\"Tokenized texts:\")\n",
    "print(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################replace emojis and emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 0.0/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 71.7/433.8 kB 653.6 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 143.4/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 143.4/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 276.5/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 368.6/433.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 433.8/433.8 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  isSarcastic  \\\n",
      "0      @0430yes i hope youre lurking rn. i want to li...            0   \n",
      "1      05 really taught me a valuable lesson I'm neve...            0   \n",
      "2      @098BERRY Never had a voice to protest, so you...            0   \n",
      "3      @0hMySt4rs Rest in peace & love to you and you...            0   \n",
      "4      100 days until Christmas!  evergreen_tree  #to...            0   \n",
      "...                                                  ...          ...   \n",
      "39775  @Zendaya I could see the makeup artists giving...            1   \n",
      "39776  @ZiggiWatkins11 Slvr... That's great name #NOT...            1   \n",
      "39777  @zoso4986 @Nero He is the fag we need but not ...            1   \n",
      "39778  Zuma sounding like Kanye West right now trying...            1   \n",
      "39779  @ZZUCRU @UWDawgPack So true. Students - stick ...            1   \n",
      "\n",
      "                                  text_without_stopwords  \n",
      "0      @ 0430yes hope youre lurking rn . want listen ...  \n",
      "1      05 really taught valuable lesson 'm never gon ...  \n",
      "2      @ 098BERRY Never voice protest , fed shit dige...  \n",
      "3                   @ 0hMySt4rs Rest peace & love family  \n",
      "4                100 days Christmas ! ðŸŒ² # soon ready yet  \n",
      "...                                                  ...  \n",
      "39775  @ Zendaya could see makeup artists giving u As...  \n",
      "39776    @ ZiggiWatkins11 Slvr ... 's great name # ðŸ˜‚ ðŸ˜‚ ðŸ˜‚  \n",
      "39777     @ zoso4986 @ Nero fag need fag deserve right .  \n",
      "39778  Zuma sounding like Kanye West right trying exp...  \n",
      "39779  @ ZZUCRU @ UWDawgPack true . Students - stick ...  \n",
      "\n",
      "[39780 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import json\n",
    "\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "df = dataset\n",
    "\n",
    "# Function to replace emojis with words\n",
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))  # Ensure emojis are separated by spaces\n",
    "\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    emoticon_dict = {\n",
    "    ':)': 'smile',\n",
    "    ':(': 'sad',\n",
    "    ':D': 'big smile',\n",
    "    ':P': 'tongue out',\n",
    "    ';)': 'wink',\n",
    "    ':O': 'surprise',\n",
    "    ':|': 'neutral',\n",
    "    ':/': 'uncertain',\n",
    "    \":'(\": 'tears of sadness',\n",
    "    \":'D\": 'tears of joy',\n",
    "    ':*': 'kiss',\n",
    "    ':@': 'angry',\n",
    "    ':x': 'mouth shut',\n",
    "    ':3': 'cute',\n",
    "    ':$': 'embarrassed',\n",
    "    \":')\": 'single tear',\n",
    "    ':p': 'tongue out'\n",
    "}\n",
    "\n",
    "\n",
    "    # #Construct regex pattern using re.escape() to escape special characters\n",
    "    # pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # # Replace emoticons using the pattern\n",
    "    # return pattern.sub(lambda match: emoticon_dict.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "    # Convert emoticon keys to lowercase\n",
    "    emoticon_dict_lower = {key.lower(): value for key, value in emoticon_dict.items()}\n",
    "\n",
    "    # Construct regex pattern using re.escape() to escape special characters\n",
    "    pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict_lower.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # Replace emoticons using the pattern\n",
    "    return pattern.sub(lambda match: emoticon_dict_lower.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply functions to replace emojis and emoticons and update DataFrame columns\n",
    "df['text'] = df['text'].apply(replace_emojis)\n",
    "df['text'] = df['text'].apply(replace_emoticons)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "data_dict = df.to_dict()\n",
    "\n",
    "with open('removedEmoji.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################# remove @s of users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removes @s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    # Define the regex pattern to match @ mentions followed by numbers\n",
    "    pattern = re.compile(r'@\\d+')\n",
    "\n",
    "    # Remove @ mentions using the pattern\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Load the DataFrame\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "df = dataset\n",
    "\n",
    "# Apply remove_user_mentions function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_user_mentions)\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "df.to_json(\"updated_data_without_mentions.json\", orient='records', lines=True)\n",
    "\n",
    "dataset = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removes @ and then replaces it with word person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    # Define the regex pattern to match @ mentions followed by numbers\n",
    "    pattern = re.compile(r'@\\d+')\n",
    "\n",
    "    # Remove @ mentions using the pattern\n",
    "    return pattern.sub('person', text)\n",
    "\n",
    "# Load the DataFrame\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "df = dataset\n",
    "\n",
    "# Apply remove_user_mentions function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_user_mentions)\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "df.to_json(\"updated_data_without_mentions.json\", orient='records', lines=True)\n",
    "\n",
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#spellcheck words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellcheckerNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB 653.6 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.2/6.8 MB 1.7 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/6.8 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.5/6.8 MB 3.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/6.8 MB 2.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.2/6.8 MB 3.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.4/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.8 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.0/6.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.1/6.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.2/6.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.3/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.4/6.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.6/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.7/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.9/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.9/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.2/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.2/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.3/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.5/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.6/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.1/6.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.1/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.2/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.2/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.3/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.9/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.2/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.5/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.1/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.4/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Split the indices of the DataFrame into two halves with stratified sampling\n",
    "indices_half1, indices_half2 = train_test_split(dataset.index, test_size=0.5, stratify=dataset['isSarcastic'], random_state=42)\n",
    "\n",
    "# Create DataFrame quarters using the selected indices\n",
    "dataset_half = dataset.loc[indices_half1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  isSarcastic\n",
      "13906  Now serving up great coffee from @topecacoffee...            0\n",
      "27947  I'm assuming Greek yogurt is just regular yogu...            1\n",
      "16807  Spread love In this Evening with Manwa Laage.....            0\n",
      "34788  RT The only benefit of dating a Srilankan girl...            1\n",
      "22679  @BudGirl555 awesome. I cannot wait. Overdue. P...            1\n",
      "...                                                  ...          ...\n",
      "36254  Thank God I still have 5 days to achieve my go...            1\n",
      "38668  when you know you typed ur password wrong but ...            1\n",
      "22865  Can't wait to celebrate my birthday in boot ca...            1\n",
      "17210  @tedcruz say a prayer cause no matter who is e...            0\n",
      "734            And I thought I was really sarcastic but.            0\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Sample 150 rows from the dataset\n",
    "sampled_dataset = dataset.sample(n=150, random_state=42)\n",
    "\n",
    "# Display the sampled dataset\n",
    "print(sampled_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "indices_half1, indices_half2 = train_test_split(dataset.index, test_size=0.5, stratify=dataset['isSarcastic'], random_state=42)\n",
    "\n",
    "# Split each half into two quarters\n",
    "indices_quarter1, indices_quarter2 = train_test_split(indices_half1, test_size=0.5, stratify=dataset.loc[indices_half1]['isSarcastic'], random_state=42)\n",
    "indices_quarter3, indices_quarter4 = train_test_split(indices_half2, test_size=0.5, stratify=dataset.loc[indices_half2]['isSarcastic'], random_state=42)\n",
    "\n",
    "# Create DataFrame quarters using the selected indices\n",
    "dataset_quarter1 = dataset.loc[indices_quarter1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sampled_dataset['text'], sampled_dataset['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_half['text'], dataset_half['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = dataset_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import pandas as pd\n",
    "\n",
    "test_text = \"This is a test #hdsjs\"\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spelling(text):\n",
    "    corrected_text = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word:\n",
    "            corrected_text.append(corrected_word)\n",
    "        else:\n",
    "            corrected_text.append(word)  # Keep the original word if no correction is found\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(correct_spelling)\n",
    "\n",
    "#corrected_text = correct_spelling(test_text)\n",
    "#print(corrected_text)\n",
    "\n",
    "# Convert DataFrame to JSON and write to a file\n",
    "dataset.to_json('processed_data.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#abreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (31824, 140)\n",
      "Shape of X_test: (7956, 140)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Read the dataset\n",
    "#dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and vectorize the training text data using Tokenizer and pad_sequences\n",
    "max_length = 140\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=2)\n",
    "\n",
    "# Display the shapes of the resulting matrices\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "\n",
    "# Tokenize and vectorize the testing text data using the same Tokenizer\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "# Display the shape of X_test\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_16 (Embedding)    (None, 140, 100)          3159400   \n",
      "                                                                 \n",
      " cu_dnnlstm_17 (CuDNNLSTM)   (None, 150)               151200    \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 64)                9664      \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,324,489\n",
      "Trainable params: 3,324,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Define the vocabulary size based on the actual number of unique words in the training data\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_length = 140\n",
    "\n",
    "optimizer = Adam(learning_rate=0.000009)\n",
    "m1 = Sequential()\n",
    "m1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "m1.add(CuDNNLSTM(units=150))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "m1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "498/498 [==============================] - 10s 18ms/step - loss: 0.6910 - accuracy: 0.5296 - val_loss: 0.6889 - val_accuracy: 0.5368\n",
      "Epoch 2/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.6882 - accuracy: 0.5349 - val_loss: 0.6869 - val_accuracy: 0.5368\n",
      "Epoch 3/20\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.6853 - accuracy: 0.5377 - val_loss: 0.6834 - val_accuracy: 0.5456\n",
      "Epoch 4/20\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.6794 - accuracy: 0.5782 - val_loss: 0.6765 - val_accuracy: 0.5900\n",
      "Epoch 5/20\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.6659 - accuracy: 0.6214 - val_loss: 0.6599 - val_accuracy: 0.6276\n",
      "Epoch 6/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.6224 - accuracy: 0.6764 - val_loss: 0.6076 - val_accuracy: 0.6781\n",
      "Epoch 7/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.5528 - accuracy: 0.7286 - val_loss: 0.5700 - val_accuracy: 0.7016\n",
      "Epoch 8/20\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.5117 - accuracy: 0.7534 - val_loss: 0.5526 - val_accuracy: 0.7139\n",
      "Epoch 9/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.4830 - accuracy: 0.7739 - val_loss: 0.5382 - val_accuracy: 0.7262\n",
      "Epoch 10/20\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.4597 - accuracy: 0.7874 - val_loss: 0.5299 - val_accuracy: 0.7333\n",
      "Epoch 11/20\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.4390 - accuracy: 0.8001 - val_loss: 0.5236 - val_accuracy: 0.7352\n",
      "Epoch 12/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.4202 - accuracy: 0.8109 - val_loss: 0.5194 - val_accuracy: 0.7422\n",
      "Epoch 13/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.4029 - accuracy: 0.8210 - val_loss: 0.5125 - val_accuracy: 0.7480\n",
      "Epoch 14/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.3868 - accuracy: 0.8290 - val_loss: 0.5125 - val_accuracy: 0.7470\n",
      "Epoch 15/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.3723 - accuracy: 0.8368 - val_loss: 0.5095 - val_accuracy: 0.7516\n",
      "Epoch 16/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.3576 - accuracy: 0.8442 - val_loss: 0.5122 - val_accuracy: 0.7550\n",
      "Epoch 17/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.3443 - accuracy: 0.8515 - val_loss: 0.5122 - val_accuracy: 0.7563\n",
      "Epoch 18/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.3314 - accuracy: 0.8592 - val_loss: 0.5165 - val_accuracy: 0.7596\n",
      "Epoch 19/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.3197 - accuracy: 0.8657 - val_loss: 0.5179 - val_accuracy: 0.7599\n",
      "Epoch 20/20\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.3082 - accuracy: 0.8721 - val_loss: 0.5247 - val_accuracy: 0.7616\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5247 - accuracy: 0.7616\n",
      "Loss: 0.5246917605400085, Accuracy: 76.16%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "m1.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = m1.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 1s 4ms/step\n",
      "Precision: 0.7418\n",
      "Recall: 0.7444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred_prob_m1 = m1.predict(X_test)\n",
    "y_val_pred_m1 = (y_val_pred_prob_m1 > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "# Assuming y_test is in binary format (0 or 1)\n",
    "y_val_true_m1 = y_test\n",
    "\n",
    "# Calculate precision and recall for binary classification\n",
    "precision_m1 = precision_score(y_val_true_m1, y_val_pred_m1)\n",
    "recall_m1 = recall_score(y_val_true_m1, y_val_pred_m1)\n",
    "\n",
    "# print the results\n",
    "print(f'Precision: {precision_m1:.4f}')\n",
    "print(f'Recall: {recall_m1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
