{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for each value in the 'isSarcastic' column:\n",
      "isSarcastic\n",
      "0    21292\n",
      "1    18488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Assuming \"is_sarcastic\" is the column you're interested in\n",
    "sarcastic_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of rows for each value in the 'isSarcastic' column:\")\n",
    "print(sarcastic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isSarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@0430yes i hope youre lurking rn. i want to li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05 really taught me a valuable lesson I'm neve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@098BERRY Never had a voice to protest, so you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@0hMySt4rs Rest in peace &amp; love to you and you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100 days until Christmas! ðŸŒ² #too soon  ready yet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  isSarcastic\n",
       "0  @0430yes i hope youre lurking rn. i want to li...            0\n",
       "1  05 really taught me a valuable lesson I'm neve...            0\n",
       "2  @098BERRY Never had a voice to protest, so you...            0\n",
       "3  @0hMySt4rs Rest in peace & love to you and you...            0\n",
       "4   100 days until Christmas! ðŸŒ² #too soon  ready yet            0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "\n",
    "#Preprocessing starts here\n",
    "\n",
    "\n",
    "#stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized texts:\n",
      "0        @0430yes i hope youre lurking rn. i want to li...\n",
      "1        05 really taught me a valuable lesson I'm neve...\n",
      "2        @098BERRY Never had a voice to protest, so you...\n",
      "3        @0hMySt4rs Rest in peace & love to you and you...\n",
      "4         100 days until Christmas! ðŸŒ² #too soon  ready yet\n",
      "                               ...                        \n",
      "39775    @Zendaya I could see the makeup artists giving...\n",
      "39776    @ZiggiWatkins11 Slvr... That's great name #NOT...\n",
      "39777    @zoso4986 @Nero He is the fag we need but not ...\n",
      "39778    Zuma sounding like Kanye West right now trying...\n",
      "39779    @ZZUCRU @UWDawgPack So true. Students - stick ...\n",
      "Name: text, Length: 39780, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply remove_stopwords function to the 'text' column\n",
    "df['text_without_stopwords'] = df['text'].apply(remove_stopwords)\n",
    "\n",
    "print(\"Tokenized texts:\")\n",
    "print(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################replace emojis and emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 0.0/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 71.7/433.8 kB 653.6 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 143.4/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 143.4/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 276.5/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 368.6/433.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 433.8/433.8 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  isSarcastic\n",
      "0      @0430yes i hope youre lurking rn. i want to li...            0\n",
      "1      05 really taught me a valuable lesson I'm neve...            0\n",
      "2      @098BERRY Never had a voice to protest, so you...            0\n",
      "3      @0hMySt4rs Rest in peace & love to you and you...            0\n",
      "4      100 days until Christmas!  evergreen_tree  #to...            0\n",
      "...                                                  ...          ...\n",
      "39775  @Zendaya I could see the makeup artists giving...            1\n",
      "39776  @ZiggiWatkins11 Slvr... That's great name #NOT...            1\n",
      "39777  @zoso4986 @Nero He is the fag we need but not ...            1\n",
      "39778  Zuma sounding like Kanye West right now trying...            1\n",
      "39779  @ZZUCRU @UWDawgPack So true. Students - stick ...            1\n",
      "\n",
      "[39780 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import json\n",
    "\n",
    "df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "\n",
    "# Function to replace emojis with words\n",
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))  # Ensure emojis are separated by spaces\n",
    "\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    emoticon_dict = {\n",
    "    ':)': 'smile',\n",
    "    ':(': 'sad',\n",
    "    ':D': 'big smile',\n",
    "    ':P': 'tongue out',\n",
    "    ';)': 'wink',\n",
    "    ':O': 'surprise',\n",
    "    ':|': 'neutral',\n",
    "    ':/': 'uncertain',\n",
    "    \":'(\": 'tears of sadness',\n",
    "    \":'D\": 'tears of joy',\n",
    "    ':*': 'kiss',\n",
    "    ':@': 'angry',\n",
    "    ':x': 'mouth shut',\n",
    "    ':3': 'cute',\n",
    "    ':$': 'embarrassed',\n",
    "    \":')\": 'single tear',\n",
    "    ':p': 'tongue out'\n",
    "}\n",
    "\n",
    "\n",
    "    # #Construct regex pattern using re.escape() to escape special characters\n",
    "    # pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # # Replace emoticons using the pattern\n",
    "    # return pattern.sub(lambda match: emoticon_dict.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "    # Convert emoticon keys to lowercase\n",
    "    emoticon_dict_lower = {key.lower(): value for key, value in emoticon_dict.items()}\n",
    "\n",
    "    # Construct regex pattern using re.escape() to escape special characters\n",
    "    pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict_lower.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # Replace emoticons using the pattern\n",
    "    return pattern.sub(lambda match: emoticon_dict_lower.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply functions to replace emojis and emoticons and update DataFrame columns\n",
    "df['text'] = df['text'].apply(replace_emojis)\n",
    "df['text'] = df['text'].apply(replace_emoticons)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "data_dict = df.to_dict()\n",
    "\n",
    "with open('removedEmoji.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################# remove @s of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    # Define the regex pattern to match @ mentions followed by numbers\n",
    "    pattern = re.compile(r'@\\d+')\n",
    "\n",
    "    # Remove @ mentions using the pattern\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Load the DataFrame\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "df = dataset\n",
    "\n",
    "# Apply remove_user_mentions function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_user_mentions)\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "df.to_json(\"updated_data_without_mentions.json\", orient='records', lines=True)\n",
    "\n",
    "dataset = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#spellcheck words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellcheckerNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB 653.6 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.2/6.8 MB 1.7 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/6.8 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.5/6.8 MB 3.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/6.8 MB 2.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.2/6.8 MB 3.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.4/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.8 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.0/6.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.1/6.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.2/6.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.3/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.4/6.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.6/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.7/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.9/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.9/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.2/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.2/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.3/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.5/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.6/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.1/6.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.1/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.2/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.2/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.3/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.9/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.2/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.5/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.1/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.4/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Split the indices of the DataFrame into two halves with stratified sampling\n",
    "indices_half1, indices_half2 = train_test_split(dataset.index, test_size=0.5, stratify=dataset['isSarcastic'], random_state=42)\n",
    "\n",
    "# Create DataFrame quarters using the selected indices\n",
    "dataset_half = dataset.loc[indices_half1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "indices_half1, indices_half2 = train_test_split(dataset.index, test_size=0.5, stratify=dataset['isSarcastic'], random_state=42)\n",
    "\n",
    "# Split each half into two quarters\n",
    "indices_quarter1, indices_quarter2 = train_test_split(indices_half1, test_size=0.5, stratify=dataset.loc[indices_half1]['isSarcastic'], random_state=42)\n",
    "indices_quarter3, indices_quarter4 = train_test_split(indices_half2, test_size=0.5, stratify=dataset.loc[indices_half2]['isSarcastic'], random_state=42)\n",
    "\n",
    "# Create DataFrame quarters using the selected indices\n",
    "dataset_quarter1 = dataset.loc[indices_quarter1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_quarter1['text'], dataset_quarter1['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = dataset_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_half['text'], dataset_half['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = dataset_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m             corrected_text\u001b[38;5;241m.\u001b[39mappend(word)  \u001b[38;5;66;03m# Keep the original word if no correction is found\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected_text)\n\u001b[1;32m---> 19\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect_spelling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#corrected_text = correct_spelling(test_text)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#print(corrected_text)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Convert DataFrame to JSON and write to a file\u001b[39;00m\n\u001b[0;32m     25\u001b[0m dataset\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4791\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m, in \u001b[0;36mcorrect_spelling\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m---> 11\u001b[0m     corrected_word \u001b[38;5;241m=\u001b[39m \u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m corrected_word:\n\u001b[0;32m     13\u001b[0m         corrected_text\u001b[38;5;241m.\u001b[39mappend(corrected_word)\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:158\u001b[0m, in \u001b[0;36mSpellChecker.correction\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The most probable correct spelling for the word\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    word (str): The word to correct\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    str: The most likely candidate or None if no correction is present\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m word \u001b[38;5;241m=\u001b[39m ensure_unicode(word)\n\u001b[1;32m--> 158\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:185\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 185\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:252\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medit_distance_1(e1))]\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[1;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:198\u001b[0m, in \u001b[0;36mSpellChecker.known\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m--> 198\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "File \u001b[1;32mc:\\Users\\Sarah\\Desktop\\4th-year-solo-project\\.conda\\lib\\site-packages\\spellchecker\\spellchecker.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m--> 198\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import pandas as pd\n",
    "\n",
    "test_text = \"This is a test #hdsjs\"\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spelling(text):\n",
    "    corrected_text = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word:\n",
    "            corrected_text.append(corrected_word)\n",
    "        else:\n",
    "            corrected_text.append(word)  # Keep the original word if no correction is found\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(correct_spelling)\n",
    "\n",
    "#corrected_text = correct_spelling(test_text)\n",
    "#print(corrected_text)\n",
    "\n",
    "# Convert DataFrame to JSON and write to a file\n",
    "dataset.to_json('processed_data.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#abreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (31824, 140)\n",
      "Shape of X_test: (7956, 140)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Read the dataset\n",
    "#dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and vectorize the training text data using Tokenizer and pad_sequences\n",
    "max_length = 140\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=2)\n",
    "\n",
    "# Display the shapes of the resulting matrices\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "\n",
    "# Tokenize and vectorize the testing text data using the same Tokenizer\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "# Display the shape of X_test\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_26 (Embedding)    (None, 140, 100)          3221100   \n",
      "                                                                 \n",
      " cu_dnnlstm_29 (CuDNNLSTM)   (None, 150)               151200    \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 64)                9664      \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,386,189\n",
      "Trainable params: 3,386,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Define the vocabulary size based on the actual number of unique words in the training data\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_length = 140\n",
    "\n",
    "m1 = Sequential()\n",
    "m1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "m1.add(CuDNNLSTM(units=150))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# m1 = Sequential()\n",
    "# m1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "# m1.add(CuDNNLSTM(units=150, return_sequences=True))  # Use return_sequences for stacked LSTM\n",
    "# m1.add(Dropout(0.2))  # Add dropout layer to reduce overfitting\n",
    "# m1.add(CuDNNLSTM(units=150))  # Increase the number of units\n",
    "# m1.add(Dense(units=64, activation='relu'))  # Use ReLU activation\n",
    "# m1.add(Dropout(0.2))  # Add dropout layer\n",
    "# m1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "m1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "498/498 [==============================] - 17s 26ms/step - loss: 0.5520 - accuracy: 0.7112 - val_loss: 0.4776 - val_accuracy: 0.7637\n",
      "Epoch 2/10\n",
      "498/498 [==============================] - 25s 50ms/step - loss: 0.3537 - accuracy: 0.8455 - val_loss: 0.4972 - val_accuracy: 0.7638\n",
      "Epoch 3/10\n",
      "498/498 [==============================] - 40s 81ms/step - loss: 0.2375 - accuracy: 0.9031 - val_loss: 0.5678 - val_accuracy: 0.7559\n",
      "Epoch 4/10\n",
      "498/498 [==============================] - 33s 65ms/step - loss: 0.1668 - accuracy: 0.9339 - val_loss: 0.6300 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "498/498 [==============================] - 43s 86ms/step - loss: 0.1238 - accuracy: 0.9517 - val_loss: 0.7947 - val_accuracy: 0.7464\n",
      "Epoch 6/10\n",
      "498/498 [==============================] - 39s 78ms/step - loss: 0.0968 - accuracy: 0.9623 - val_loss: 0.8092 - val_accuracy: 0.7379\n",
      "Epoch 7/10\n",
      "498/498 [==============================] - 37s 74ms/step - loss: 0.0758 - accuracy: 0.9713 - val_loss: 1.0710 - val_accuracy: 0.7348\n",
      "Epoch 8/10\n",
      "498/498 [==============================] - 33s 66ms/step - loss: 0.0629 - accuracy: 0.9754 - val_loss: 1.0134 - val_accuracy: 0.7369\n",
      "Epoch 9/10\n",
      "498/498 [==============================] - 45s 91ms/step - loss: 0.0544 - accuracy: 0.9795 - val_loss: 1.0783 - val_accuracy: 0.7296\n",
      "Epoch 10/10\n",
      "498/498 [==============================] - 26s 53ms/step - loss: 0.0458 - accuracy: 0.9827 - val_loss: 1.3018 - val_accuracy: 0.7279\n",
      "249/249 [==============================] - 13s 53ms/step - loss: 1.3018 - accuracy: 0.7279\n",
      "Loss: 1.3017746210098267, Accuracy: 72.79%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "m1.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = m1.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 29s 117ms/step\n",
      "Precision: 0.6870\n",
      "Recall: 0.7577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred_prob_m1 = m1.predict(X_test)\n",
    "y_val_pred_m1 = (y_val_pred_prob_m1 > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "# Assuming y_test is in binary format (0 or 1)\n",
    "y_val_true_m1 = y_test\n",
    "\n",
    "# Calculate precision and recall for binary classification\n",
    "precision_m1 = precision_score(y_val_true_m1, y_val_pred_m1)\n",
    "recall_m1 = recall_score(y_val_true_m1, y_val_pred_m1)\n",
    "\n",
    "# print the results\n",
    "print(f'Precision: {precision_m1:.4f}')\n",
    "print(f'Recall: {recall_m1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
