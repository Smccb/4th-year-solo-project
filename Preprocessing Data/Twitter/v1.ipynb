{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for each value in the 'isSarcastic' column:\n",
      "isSarcastic\n",
      "0    21292\n",
      "1    18488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Assuming \"is_sarcastic\" is the column you're interested in\n",
    "sarcastic_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of rows for each value in the 'isSarcastic' column:\")\n",
    "print(sarcastic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for each value in the 'isSarcastic' column:\n",
      "isSarcastic\n",
      "0    18488\n",
      "1    18488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#update number of rows in dataset undersampling\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Count the number of instances in each class\n",
    "class_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Find the class with more items\n",
    "majority_class = class_counts.idxmax()\n",
    "\n",
    "# Find the class with fewer items\n",
    "minority_class = class_counts.idxmin()\n",
    "\n",
    "# Count the number of instances in the minority class\n",
    "minority_class_count = class_counts[minority_class]\n",
    "\n",
    "# Sample the majority class to match the number of instances in the minority class\n",
    "majority_class_sampled = dataset[dataset['isSarcastic'] == majority_class].sample(n=minority_class_count, random_state=42)\n",
    "\n",
    "# Concatenate the sampled majority class with the minority class\n",
    "balanced_data = pd.concat([majority_class_sampled, dataset[dataset['isSarcastic'] == minority_class]])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "dataset = balanced_data\n",
    "# Now, 'balanced_data' contains a balanced dataset where both classes have the same number of instances\n",
    "\n",
    "sarcastic_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of rows for each value in the 'isSarcastic' column:\")\n",
    "print(sarcastic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#half the dataset and undersample\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'data' with columns 'is_sarcastic' and other features\n",
    "\n",
    "# Count the number of instances in each class\n",
    "class_counts = data['isSarcastic'].value_counts()\n",
    "\n",
    "# Find the class with more items\n",
    "majority_class = class_counts.idxmax()\n",
    "\n",
    "# Find the class with fewer items\n",
    "minority_class = class_counts.idxmin()\n",
    "\n",
    "# Count the number of instances in the minority class\n",
    "minority_class_count = class_counts[minority_class]\n",
    "\n",
    "# Sample the majority class to match the number of instances in the minority class\n",
    "majority_class_sampled = data[data['isSarcastic'] == majority_class].sample(n=minority_class_count, random_state=42)\n",
    "\n",
    "# Concatenate the sampled majority class with the minority class\n",
    "balanced_data = pd.concat([majority_class_sampled, data[data['isSarcastic'] == minority_class]])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the balanced dataset into two equal parts\n",
    "half_index = len(balanced_data) // 2\n",
    "balanced_data_part1 = balanced_data.iloc[:half_index]\n",
    "balanced_data_part2 = balanced_data.iloc[half_index:]\n",
    "\n",
    "# Now, 'balanced_data_part1' and 'balanced_data_part2' contain two equal parts of the balanced dataset\n",
    "dataset = balanced_data_part1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isSarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I hate  two -faced  people. It's so hard to de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What a day for Cyrus Jones!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter has a bird as its logo, that's why whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@arden_cho How does it feel to know that peopl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@BlackIrishI @Dolly0811 @akawhit1 @sheila14all...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  isSarcastic\n",
       "0  I hate  two -faced  people. It's so hard to de...            0\n",
       "1                        What a day for Cyrus Jones!            0\n",
       "2  Twitter has a bird as its logo, that's why whe...            1\n",
       "3  @arden_cho How does it feel to know that peopl...            0\n",
       "4  @BlackIrishI @Dolly0811 @akawhit1 @sheila14all...            0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "\n",
    "#Preprocessing starts here\n",
    "\n",
    "\n",
    "#stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized texts:\n",
      "0        I hate  two -faced  people. It's so hard to de...\n",
      "1                              What a day for Cyrus Jones!\n",
      "2        Twitter has a bird as its logo, that's why whe...\n",
      "3        @arden_cho How does it feel to know that peopl...\n",
      "4        @BlackIrishI @Dolly0811 @akawhit1 @sheila14all...\n",
      "                               ...                        \n",
      "18483    many happy returnS Steven, sending best wishes...\n",
      "18484        just the pitter-patter of rainfall in my mind\n",
      "18485    @italiaricci @scottmfoster in having Chasing L...\n",
      "18486    @amyschumer do yourself a favor and delete you...\n",
      "18487    Look I'm sorry about your lamp but automatic k...\n",
      "Name: text, Length: 18488, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply remove_stopwords function to the 'text' column\n",
    "df['text_without_stopwords'] = df['text'].apply(remove_stopwords)\n",
    "\n",
    "print(\"Tokenized texts:\")\n",
    "print(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################replace emojis and emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 0.0/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 71.7/433.8 kB 653.6 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 143.4/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 143.4/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 276.5/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 368.6/433.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 433.8/433.8 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  isSarcastic\n",
      "0      I hate  two -faced  people. It's so hard to de...            0\n",
      "1                            What a day for Cyrus Jones!            0\n",
      "2      Twitter has a bird as its logo, that's why whe...            1\n",
      "3      @arden_cho How does it feel to know that peopl...            0\n",
      "4      @BlackIrishI @Dolly0811 @akawhit1 @sheila14all...            0\n",
      "...                                                  ...          ...\n",
      "36971                      well there goes my happy mood            0\n",
      "36972                                   Eyebrow twitches            0\n",
      "36973  Preliminary autopsies show a mother and father...            0\n",
      "36974  Why wait to live life if you only live once? t...            0\n",
      "36975  when I have a crap load of homework, subhan al...            0\n",
      "\n",
      "[36976 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import json\n",
    "\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "df = dataset\n",
    "\n",
    "# Function to replace emojis with words\n",
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))  # Ensure emojis are separated by spaces\n",
    "\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    emoticon_dict = {\n",
    "    ':)': 'smile',\n",
    "    ':(': 'frown',\n",
    "    ':D': 'big smile',\n",
    "    ':P': 'tongue out',\n",
    "    ';)': 'wink',\n",
    "    ':O': 'surprise',\n",
    "    ':|': 'neutral',\n",
    "    ':/': 'uncertain',\n",
    "    \":'(\": 'tears of sadness',\n",
    "    \":'D\": 'tears of joy',\n",
    "    ':*': 'kiss',\n",
    "    ':@': 'angry',\n",
    "    ':x': 'mouth shut',\n",
    "    ':3': 'cute',\n",
    "    ':$': 'embarrassed',\n",
    "    \":')\": 'single tear',\n",
    "    ':p': 'tongue out'\n",
    "}\n",
    "\n",
    "    emoticon_dict_lower = {key.lower(): value for key, value in emoticon_dict.items()}\n",
    "\n",
    "    pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict_lower.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    return pattern.sub(lambda match: emoticon_dict_lower.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply functions to replace emojis and emoticons and update DataFrame columns\n",
    "df['text'] = df['text'].apply(replace_emojis)\n",
    "df['text'] = df['text'].apply(replace_emoticons)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "data_dict = df.to_dict()\n",
    "\n",
    "with open('removedEmoji.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    emoticon_dict = {\n",
    "        ':)': 'smile',\n",
    "        ':(': 'frown',\n",
    "        ':D': 'big smile',\n",
    "        ':P': 'tongue out',\n",
    "        ';)': 'wink',\n",
    "        ':O': 'surprise',\n",
    "        ':|': 'neutral',\n",
    "        ':/': 'uncertain',\n",
    "        \":'(\": 'tears of sadness',\n",
    "        \":'D\": 'tears of joy',\n",
    "        ':*': 'kiss',\n",
    "        ':@': 'angry',\n",
    "        ':x': 'mouth shut',\n",
    "        ':3': 'cute',\n",
    "        ':$': 'embarrassed',\n",
    "        \":')\": 'single tear',\n",
    "        ':p': 'tongue out'\n",
    "    }\n",
    "    emoticon_dict_lower = {key.lower(): value for key, value in emoticon_dict.items()}\n",
    "    \n",
    "    pattern = re.compile(r\"[:;]['-]?[)DPO|/\\\\@x3*$p]\", re.IGNORECASE)\n",
    "\n",
    "    def replace_func(match):\n",
    "        return emoticon_dict_lower.get(match.group().lower(), match.group())\n",
    "\n",
    "    return pattern.sub(replace_func, text)\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(replace_emoticons)\n",
    "\n",
    "data_dict = df.to_dict(orient='records')\n",
    "\n",
    "# Save the dictionary to a json\n",
    "with open('removedEmoji2.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################# remove @s of users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removes @s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    # Define the regex pattern to match @ mentions followed by numbers\n",
    "    pattern = re.compile(r'@\\d+')\n",
    "\n",
    "    # Remove @ mentions using the pattern\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Load the DataFrame\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "df = dataset\n",
    "\n",
    "# Apply remove_user_mentions function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_user_mentions)\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "df.to_json(\"updated_data_without_mentions.json\", orient='records', lines=True)\n",
    "\n",
    "dataset = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removes @ and then replaces it with word person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    # Define the regex pattern to match @ mentions followed by numbers\n",
    "    pattern = re.compile(r'@\\d+')\n",
    "\n",
    "    # Remove @ mentions using the pattern\n",
    "    return pattern.sub('person', text)\n",
    "\n",
    "# Load the DataFrame\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "df = dataset\n",
    "\n",
    "# Apply remove_user_mentions function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_user_mentions)\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "df.to_json(\"updated_data_without_mentions.json\", orient='records', lines=True)\n",
    "\n",
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#spellcheck words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellcheckerNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB 653.6 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.2/6.8 MB 1.7 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/6.8 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.5/6.8 MB 3.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/6.8 MB 2.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.2/6.8 MB 3.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.4/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.8 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.0/6.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.1/6.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.2/6.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.3/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.4/6.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.6/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.7/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.9/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.9/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.2/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.2/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.3/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.5/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.6/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.1/6.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.1/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.2/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.2/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.3/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.9/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.2/6.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.5/6.8 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.1/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.4/6.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Split the indices of the DataFrame into two halves with stratified sampling\n",
    "indices_half1, indices_half2 = train_test_split(dataset.index, test_size=0.5, stratify=dataset['isSarcastic'], random_state=42)\n",
    "\n",
    "# Create DataFrame quarters using the selected indices\n",
    "dataset_half = dataset.loc[indices_half1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  isSarcastic\n",
      "13906  Now serving up great coffee from @topecacoffee...            0\n",
      "27947  I'm assuming Greek yogurt is just regular yogu...            1\n",
      "16807  Spread love In this Evening with Manwa Laage.....            0\n",
      "34788  RT The only benefit of dating a Srilankan girl...            1\n",
      "22679  @BudGirl555 awesome. I cannot wait. Overdue. P...            1\n",
      "...                                                  ...          ...\n",
      "36254  Thank God I still have 5 days to achieve my go...            1\n",
      "38668  when you know you typed ur password wrong but ...            1\n",
      "22865  Can't wait to celebrate my birthday in boot ca...            1\n",
      "17210  @tedcruz say a prayer cause no matter who is e...            0\n",
      "734            And I thought I was really sarcastic but.            0\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Sample 150 rows from the dataset\n",
    "sampled_dataset = dataset.sample(n=150, random_state=42)\n",
    "\n",
    "# Display the sampled dataset\n",
    "print(sampled_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "indices_half1, indices_half2 = train_test_split(dataset.index, test_size=0.5, stratify=dataset['isSarcastic'], random_state=42)\n",
    "\n",
    "# Split each half into two quarters\n",
    "indices_quarter1, indices_quarter2 = train_test_split(indices_half1, test_size=0.5, stratify=dataset.loc[indices_half1]['isSarcastic'], random_state=42)\n",
    "indices_quarter3, indices_quarter4 = train_test_split(indices_half2, test_size=0.5, stratify=dataset.loc[indices_half2]['isSarcastic'], random_state=42)\n",
    "\n",
    "# Create DataFrame quarters using the selected indices\n",
    "dataset_quarter1 = dataset.loc[indices_quarter1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sampled_dataset['text'], sampled_dataset['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_half['text'], dataset_half['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = dataset_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import pandas as pd\n",
    "\n",
    "test_text = \"This is a test #hdsjs\"\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spelling(text):\n",
    "    corrected_text = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word:\n",
    "            corrected_text.append(corrected_word)\n",
    "        else:\n",
    "            corrected_text.append(word)  # Keep the original word if no correction is found\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(correct_spelling)\n",
    "\n",
    "#corrected_text = correct_spelling(test_text)\n",
    "#print(corrected_text)\n",
    "\n",
    "# Convert DataFrame to JSON and write to a file\n",
    "dataset.to_json('processed_data.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#abreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Example mapping of abbreviations to their full forms\n",
    "abbreviation_mapping = {\n",
    "    'OMG': 'oh my god',\n",
    "    'DM': 'direct message',\n",
    "    'BTW': 'by the way',\n",
    "    'BRB': 'be right back',\n",
    "    'RT': 'retweet',\n",
    "    'FTW': 'for the win',\n",
    "    'QOTD': 'quote of the day',\n",
    "    'IDK': 'I do not know',\n",
    "    'ICYMI': 'in case you missed it',\n",
    "    'IRL': 'in real life',\n",
    "    'IMHO': 'in my humble opinion',\n",
    "    'IMO': 'I do not know',\n",
    "    'LOL': 'laugh out loud',\n",
    "    'LMAO': 'laughing my ass off',\n",
    "    'LMFAO': 'laughing my fucking ass off',\n",
    "    'NTS': 'note to self',\n",
    "    'F2F': 'face to face',\n",
    "    'B4': 'before',\n",
    "    'DM': 'direct message',\n",
    "    'CC': 'carbon copy',\n",
    "    'SMH': 'shaking my head',\n",
    "    'STFU': 'shut the fuck up',\n",
    "    'BFN': 'by for now',\n",
    "    'AFAIK': 'as far as I know',\n",
    "    'TY': 'thank you',\n",
    "    'YW': 'you are welcome',\n",
    "    'THX': 'thanks'\n",
    "}\n",
    "\n",
    "# Function to replace abbreviations\n",
    "def replace_abbreviations(text):\n",
    "    tokens = text.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.upper() in abbreviation_mapping:\n",
    "            tokens[i] = abbreviation_mapping[token.upper()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply functions to remove hashtags and replace abbreviations to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(lambda x: x.upper())  # Convert text to uppercase\n",
    "dataset['text'] = dataset['text'].apply(replace_abbreviations)\n",
    "\n",
    "# Restore original capitalization\n",
    "original_capitalization = lambda x: ''.join([a if b.islower() else a.lower() for a, b in zip(x, dataset['text'][0])])\n",
    "dataset['text'] = dataset['text'].apply(original_capitalization)\n",
    "\n",
    "# Save the updated DataFrame to a JSON file\n",
    "dataset.to_json('abbreviations_removed.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh my god, I didn't know that! by the way, are we meeting in real life or face to face?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "abbreviation_mapping = {\n",
    "    'OMG': 'oh my god',\n",
    "    'DM': 'direct message',\n",
    "    'BTW': 'by the way',\n",
    "    'BRB': 'be right back',\n",
    "    'RT': 'retweet',\n",
    "    'FTW': 'for the win',\n",
    "    'QOTD': 'quote of the day',\n",
    "    'IDK': 'I do not know',\n",
    "    'ICYMI': 'in case you missed it',\n",
    "    'IRL': 'in real life',\n",
    "    'IMHO': 'in my humble opinion',\n",
    "    'IMO': 'I do not know',\n",
    "    'LOL': 'laugh out loud',\n",
    "    'LMAO': 'laughing my ass off',\n",
    "    'LMFAO': 'laughing my fucking ass off',\n",
    "    'NTS': 'note to self',\n",
    "    'F2F': 'face to face',\n",
    "    'B4': 'before',\n",
    "    'CC': 'carbon copy',\n",
    "    'SMH': 'shaking my head',\n",
    "    'STFU': 'shut the fuck up',\n",
    "    'BFN': 'by for now',\n",
    "    'AFAIK': 'as far as I know',\n",
    "    'TY': 'thank you',\n",
    "    'YW': 'you are welcome',\n",
    "    'THX': 'thanks'\n",
    "}\n",
    "\n",
    "# Function to replace abbreviations using regex\n",
    "def replace_abbreviations(text):\n",
    "    # Create a regex pattern that matches any abbreviation in the mapping, case-insensitively\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(abbreviation) for abbreviation in abbreviation_mapping.keys()) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    # Function to replace each abbreviation with its full form\n",
    "    def replace(match):\n",
    "        # Use the matched text to look up the full form, preserving the original case\n",
    "        return abbreviation_mapping[match.group().upper()]\n",
    "\n",
    "    # Replace all matches in the text\n",
    "    return pattern.sub(replace, text)\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"OMG, I didn't know that! BTW, are we meeting IRL or F2F?\"\n",
    "processed_text = replace_abbreviations(sample_text)\n",
    "\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#remove all hashtags and don't replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Example DataFrame with 'text' column containing Twitter data\n",
    "#dataset = pd.DataFrame({'text': [\"This is a tweet with #hashtags\", \"Another tweet with #morehashtags\", \"Yet another tweet with #hashtags\"]})\n",
    "\n",
    "# Function to remove hashtags from a single text\n",
    "def remove_hashtags(text):\n",
    "    pattern = r'\\#\\w+'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# Apply the function to remove hashtags to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(remove_hashtags)\n",
    "\n",
    "\n",
    "data_dict = dataset.to_dict()\n",
    "\n",
    "with open('all#Removed.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "#unicode escapes fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_unicode(text):\n",
    "    # Decode Unicode escape sequences using 'unicode_escape' codec\n",
    "    decoded_text = text.encode().decode('unicode_escape')\n",
    "    # Encode the resulting Unicode string using 'latin-1' codec, ignoring errors\n",
    "    encoded_text = decoded_text.encode('latin-1', 'ignore')\n",
    "    return encoded_text.decode('utf-8')\n",
    "\n",
    "\n",
    "# Apply the function to each element in the dataset\n",
    "fixed_dataset = [decode_unicode(text) for text in dataset]\n",
    "\n",
    "with open('unicodeFixes.json', 'w') as f:\n",
    "    json.dump(fixed_dataset, f, indent=4)\n",
    "\n",
    "dataset = fixed_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (31824, 140)\n",
      "Shape of X_test: (7956, 140)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Read the dataset\n",
    "#dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['isSarcastic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and vectorize the training text data using Tokenizer and pad_sequences\n",
    "max_length = 140\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=2)\n",
    "\n",
    "# Display the shapes of the resulting matrices\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "\n",
    "# Tokenize and vectorize the testing text data using the same Tokenizer\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "# Display the shape of X_test\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 140, 100)          2731700   \n",
      "                                                                 \n",
      " cu_dnnlstm (CuDNNLSTM)      (None, 150)               151200    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                9664      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,896,789\n",
      "Trainable params: 2,896,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Define the vocabulary size based on the actual number of unique words in the training data\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_length = 140\n",
    "\n",
    "optimizer = Adam(learning_rate=0.000009)\n",
    "m1 = Sequential()\n",
    "m1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "m1.add(CuDNNLSTM(units=150))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "m1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "498/498 [==============================] - 12s 17ms/step - loss: 0.6911 - accuracy: 0.5324 - val_loss: 0.6890 - val_accuracy: 0.5368\n",
      "Epoch 2/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.6883 - accuracy: 0.5348 - val_loss: 0.6868 - val_accuracy: 0.5368\n",
      "Epoch 3/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.6855 - accuracy: 0.5361 - val_loss: 0.6834 - val_accuracy: 0.5411\n",
      "Epoch 4/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.6798 - accuracy: 0.5698 - val_loss: 0.6767 - val_accuracy: 0.5816\n",
      "Epoch 5/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6580 - val_accuracy: 0.6225\n",
      "Epoch 6/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.6188 - accuracy: 0.6731 - val_loss: 0.6043 - val_accuracy: 0.6810\n",
      "Epoch 7/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.5594 - accuracy: 0.7183 - val_loss: 0.5787 - val_accuracy: 0.6961\n",
      "Epoch 8/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.5246 - accuracy: 0.7431 - val_loss: 0.5623 - val_accuracy: 0.7107\n",
      "Epoch 9/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.4977 - accuracy: 0.7626 - val_loss: 0.5526 - val_accuracy: 0.7188\n",
      "Epoch 10/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.4749 - accuracy: 0.7786 - val_loss: 0.5371 - val_accuracy: 0.7305\n",
      "Epoch 11/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.4547 - accuracy: 0.7899 - val_loss: 0.5281 - val_accuracy: 0.7340\n",
      "Epoch 12/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.4368 - accuracy: 0.8005 - val_loss: 0.5246 - val_accuracy: 0.7391\n",
      "Epoch 13/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.4205 - accuracy: 0.8083 - val_loss: 0.5223 - val_accuracy: 0.7438\n",
      "Epoch 14/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.4053 - accuracy: 0.8167 - val_loss: 0.5192 - val_accuracy: 0.7462\n",
      "Epoch 15/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.3915 - accuracy: 0.8246 - val_loss: 0.5184 - val_accuracy: 0.7471\n",
      "Epoch 16/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.3782 - accuracy: 0.8326 - val_loss: 0.5171 - val_accuracy: 0.7496\n",
      "Epoch 17/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.3660 - accuracy: 0.8403 - val_loss: 0.5218 - val_accuracy: 0.7494\n",
      "Epoch 18/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.3543 - accuracy: 0.8460 - val_loss: 0.5234 - val_accuracy: 0.7533\n",
      "Epoch 19/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.3431 - accuracy: 0.8522 - val_loss: 0.5242 - val_accuracy: 0.7541\n",
      "Epoch 20/20\n",
      "498/498 [==============================] - 8s 16ms/step - loss: 0.3326 - accuracy: 0.8562 - val_loss: 0.5288 - val_accuracy: 0.7536\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5288 - accuracy: 0.7535\n",
      "Loss: 0.5287942886352539, Accuracy: 75.35%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "m1.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = m1.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 1s 6ms/step\n",
      "Precision: 0.7173\n",
      "Recall: 0.6654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred_prob_m1 = m1.predict(X_test)\n",
    "y_val_pred_m1 = (y_val_pred_prob_m1 > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "# Assuming y_test is in binary format (0 or 1)\n",
    "y_val_true_m1 = y_test\n",
    "\n",
    "# Calculate precision and recall for binary classification\n",
    "precision_m1 = precision_score(y_val_true_m1, y_val_pred_m1)\n",
    "recall_m1 = recall_score(y_val_true_m1, y_val_pred_m1)\n",
    "\n",
    "# print the results\n",
    "print(f'Precision: {precision_m1:.4f}')\n",
    "print(f'Recall: {recall_m1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
