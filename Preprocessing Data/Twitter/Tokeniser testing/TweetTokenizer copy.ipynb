{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_json(\"../data_without_hashtags.json\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['isSarcastic'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(\"../data_without_hashtags.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#Preprocessing section here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for each value in the 'isSarcastic' column:\n",
      "isSarcastic\n",
      "0    18488\n",
      "1    18488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#undersampling here\n",
    "\n",
    "# Count the number of instances in each class\n",
    "class_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Find the class with more items\n",
    "majority_class = class_counts.idxmax()\n",
    "\n",
    "# Find the class with fewer items\n",
    "minority_class = class_counts.idxmin()\n",
    "\n",
    "# Count the number of instances in the minority class\n",
    "minority_class_count = class_counts[minority_class]\n",
    "\n",
    "# Sample the majority class to match the number of instances in the minority class\n",
    "majority_class_sampled = dataset[dataset['isSarcastic'] == majority_class].sample(n=minority_class_count, random_state=42)\n",
    "\n",
    "# Concatenate the sampled majority class with the minority class\n",
    "balanced_data = pd.concat([majority_class_sampled, dataset[dataset['isSarcastic'] == minority_class]])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "dataset = balanced_data\n",
    "# Now, 'balanced_data' contains a balanced dataset where both classes have the same number of instances\n",
    "\n",
    "sarcastic_counts = dataset['isSarcastic'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of rows for each value in the 'isSarcastic' column:\")\n",
    "print(sarcastic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#remove @ with Person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    # Define the regex pattern to match @ mentions followed by numbers\n",
    "    pattern = re.compile(r'@\\d+')\n",
    "\n",
    "    # Remove @ mentions using the pattern\n",
    "    return pattern.sub('person', text)\n",
    "\n",
    "# Load the DataFrame\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "\n",
    "# Apply remove_user_mentions function to the 'text' column\n",
    "dataset['text'] = dataset['text'].apply(remove_user_mentions)\n",
    "\n",
    "# Save the updated DataFrame if needed\n",
    "dataset.to_json(\"updated_data_without_mentions.json\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#replace abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "# Example mapping of abbreviations to their full forms\n",
    "abbreviation_mapping = {\n",
    "    'OMG': 'oh my god',\n",
    "    'DM': 'direct message',\n",
    "    'BTW': 'by the way',\n",
    "    'BRB': 'be right back',\n",
    "    'RT': 'retweet',\n",
    "    'FTW': 'for the win',\n",
    "    'QOTD': 'quote of the day',\n",
    "    'IDK': 'I do not know',\n",
    "    'ICYMI': 'in case you missed it',\n",
    "    'IRL': 'in real life',\n",
    "    'IMHO': 'in my humble opinion',\n",
    "    'IMO': 'I do not know',\n",
    "    'LOL': 'laugh out loud',\n",
    "    'LMAO': 'laugh my ass off',\n",
    "    'NTS': 'note to self',\n",
    "    'F2F': 'face to face',\n",
    "    'B4': 'before',\n",
    "    'DM': 'direct message',\n",
    "    'CC': 'carbon copy',\n",
    "    'SMH': 'shaking my head',\n",
    "    'STFU': 'shut the fuck up',\n",
    "    'BFN': 'by for now',\n",
    "    'AFAIK': 'as far as I know',\n",
    "    'TY': 'thank you',\n",
    "    'YW': 'you are welcome',\n",
    "    'THX': 'thanks',\n",
    "    'TIL': 'today I learned',\n",
    "    'AMA': 'ask me anything',\n",
    "    'JK': 'just kidding',\n",
    "    'NSFW': 'Not Safe for Work',\n",
    "    'OOTD': 'outfit of the day',\n",
    "    'TLDR': 'too long did not read',\n",
    "    'TL;DR': 'too long; did not read',\n",
    "    'GIF': 'graphics interchange format'\n",
    "}\n",
    "\n",
    "# Function to replace abbreviations\n",
    "def replace_abbreviations(text):\n",
    "    tokens = text.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.upper() in abbreviation_mapping:\n",
    "            tokens[i] = abbreviation_mapping[token.upper()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply functions to remove hashtags and replace abbreviations to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(lambda x: x.upper())  # Convert text to uppercase\n",
    "dataset['text'] = dataset['text'].apply(replace_abbreviations)\n",
    "\n",
    "# Restore original capitalization\n",
    "original_capitalization = lambda x: ''.join([a if b.islower() else a.lower() for a, b in zip(x, dataset['text'][0])])\n",
    "dataset['text'] = dataset['text'].apply(original_capitalization)\n",
    "\n",
    "# Save the updated DataFrame to a JSON file\n",
    "dataset.to_json('abbreviations_removed.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#hashtag removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Example DataFrame with 'text' column containing Twitter data\n",
    "#dataset = pd.DataFrame({'text': [\"This is a tweet with #hashtags\", \"Another tweet with #morehashtags\", \"Yet another tweet with #hashtags\"]})\n",
    "\n",
    "# Function to remove hashtags from a single text\n",
    "def remove_hashtags(text):\n",
    "    pattern = r'\\#\\w+'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "# Apply the function to remove hashtags to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(remove_hashtags)\n",
    "\n",
    "\n",
    "data_dict = dataset.to_dict()\n",
    "\n",
    "with open('all#Removed.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#replace emji and emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  isSarcastic\n",
      "0      personyes i hope youre lurking rn. i want to l...            0\n",
      "1      05 really taught me a valuable lesson i'm neve...            0\n",
      "2      personberry never had a voice to protest, so y...            0\n",
      "3      personhmyst4rs rest in peace & love to you and...            0\n",
      "4      100 days until christmas!  evergreen_tree  #to...            0\n",
      "...                                                  ...          ...\n",
      "39775  @zendaya i could see the makeup artists giving...            1\n",
      "39776  @ziggiwatkins11 slvr... that's great name #not...            1\n",
      "39777  @zoso4986 @nero he is the fag we need but not ...            1\n",
      "39778  zuma sounding like kanye west right now trying...            1\n",
      "39779  @zzucru @uwdawgpack so true. students - stick ...            1\n",
      "\n",
      "[39780 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import json\n",
    "\n",
    "#df = pd.read_json(\"data_without_hashtags.json\")\n",
    "df = dataset\n",
    "\n",
    "# Function to replace emojis with words\n",
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))  # Ensure emojis are separated by spaces\n",
    "\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    emoticon_dict = {\n",
    "    ':)': 'smile',\n",
    "    ':(': 'frown',\n",
    "    ':D': 'big smile',\n",
    "    ':P': 'tongue out',\n",
    "    ';)': 'wink',\n",
    "    ':O': 'surprise',\n",
    "    ':|': 'neutral',\n",
    "    ':/': 'uncertain',\n",
    "    \":'(\": 'tears of sadness',\n",
    "    \":'D\": 'tears of joy',\n",
    "    ':*': 'kiss',\n",
    "    ':@': 'angry',\n",
    "    ':x': 'mouth shut',\n",
    "    ':3': 'cute',\n",
    "    ':$': 'embarrassed',\n",
    "    \":')\": 'single tear',\n",
    "    ':p': 'tongue out'\n",
    "}\n",
    "\n",
    "\n",
    "    # #Construct regex pattern using re.escape() to escape special characters\n",
    "    # pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # # Replace emoticons using the pattern\n",
    "    # return pattern.sub(lambda match: emoticon_dict.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "    # Convert emoticon keys to lowercase\n",
    "    emoticon_dict_lower = {key.lower(): value for key, value in emoticon_dict.items()}\n",
    "\n",
    "    # Construct regex pattern using re.escape() to escape special characters\n",
    "    pattern = re.compile(r'(' + '|'.join(re.escape(emoticon) for emoticon in emoticon_dict_lower.keys()) + ')', re.IGNORECASE)\n",
    "\n",
    "    # Replace emoticons using the pattern\n",
    "    return pattern.sub(lambda match: emoticon_dict_lower.get(match.group().lower(), match.group()), text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply functions to replace emojis and emoticons and update DataFrame columns\n",
    "df['text'] = df['text'].apply(replace_emojis)\n",
    "df['text'] = df['text'].apply(replace_emoticons)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "data_dict = df.to_dict()\n",
    "\n",
    "with open('removedEmoji.json', 'w') as f:\n",
    "    json.dump(data_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.1.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "   ---------------------------------------- 0.0/289.9 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/289.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 122.9/289.9 kB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 256.0/289.9 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 289.9/289.9 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading pyahocorasick-2.1.0-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "\n",
    "# Sample DataFrame\n",
    "# dataset = pd.DataFrame({\n",
    "#     'text': [\"I didn't go to the party yesterday.\",\n",
    "#              \"She can't believe what happened.\"]\n",
    "# })\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Apply the function to expand contractions\n",
    "dataset['text'] = dataset['text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (31824, 140)\n",
      "Shape of X_test: (7956, 140)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Read the dataset\n",
    "# dataset = pd.read_json(\"data_without_hashtags.json\")\n",
    "\n",
    "# Example data\n",
    "#X_train = [\"This is a tweet!\", \"Another tweet here.\"]\n",
    "#X_test = [\"Yet another tweet!\", \"And one more tweet.\"]\n",
    "\n",
    "# Define max_length\n",
    "max_length = 140\n",
    "\n",
    "# Initialize TweetTokenizer\n",
    "tweetTokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize training text data\n",
    "X_train_tokenized = [tweetTokenizer.tokenize(text) for text in X_train]\n",
    "\n",
    "# Tokenize testing text data\n",
    "X_test_tokenized = [tweetTokenizer.tokenize(text) for text in X_test]\n",
    "\n",
    "# Create Tokenizer instance\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer on training text data\n",
    "tokenizer.fit_on_texts(X_train_tokenized)\n",
    "\n",
    "# Convert text data to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_tokenized)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_tokenized)\n",
    "\n",
    "# Pad sequences\n",
    "X_train = pad_sequences(X_train_sequences, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test_sequences, maxlen=max_length)\n",
    "\n",
    "# Display shapes of resulting matrices\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    (None, 140, 100)          3342900   \n",
      "                                                                 \n",
      " cu_dnnlstm_14 (CuDNNLSTM)   (None, 150)               151200    \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 64)                9664      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,507,989\n",
      "Trainable params: 3,507,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Define the vocabulary size based on the actual number of unique words in the training data\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_length = 140\n",
    "\n",
    "optimizer = Adam(learning_rate=0.000009)\n",
    "m1 = Sequential()\n",
    "m1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "m1.add(CuDNNLSTM(units=150))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=64))\n",
    "m1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "m1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "498/498 [==============================] - 10s 19ms/step - loss: 0.5358 - accuracy: 0.7236 - val_loss: 0.4676 - val_accuracy: 0.7741\n",
      "Epoch 2/10\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.3410 - accuracy: 0.8534 - val_loss: 0.4746 - val_accuracy: 0.7746\n",
      "Epoch 3/10\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.2298 - accuracy: 0.9080 - val_loss: 0.5707 - val_accuracy: 0.7616\n",
      "Epoch 4/10\n",
      "498/498 [==============================] - 9s 17ms/step - loss: 0.1644 - accuracy: 0.9345 - val_loss: 0.6846 - val_accuracy: 0.7578\n",
      "Epoch 5/10\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.1179 - accuracy: 0.9539 - val_loss: 0.7419 - val_accuracy: 0.7528\n",
      "Epoch 6/10\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.0925 - accuracy: 0.9637 - val_loss: 0.8697 - val_accuracy: 0.7474\n",
      "Epoch 7/10\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.0748 - accuracy: 0.9715 - val_loss: 0.9718 - val_accuracy: 0.7452\n",
      "Epoch 8/10\n",
      "498/498 [==============================] - 9s 19ms/step - loss: 0.0633 - accuracy: 0.9758 - val_loss: 0.9718 - val_accuracy: 0.7460\n",
      "Epoch 9/10\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.0499 - accuracy: 0.9811 - val_loss: 1.0994 - val_accuracy: 0.7392\n",
      "Epoch 10/10\n",
      "498/498 [==============================] - 9s 18ms/step - loss: 0.0447 - accuracy: 0.9825 - val_loss: 1.2368 - val_accuracy: 0.7421\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 1.2368 - accuracy: 0.7421\n",
      "Loss: 1.236772894859314, Accuracy: 74.21%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "m1.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = m1.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 1s 5ms/step\n",
      "Precision: 0.7080\n",
      "Recall: 0.7541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred_prob_m1 = m1.predict(X_test)\n",
    "y_val_pred_m1 = (y_val_pred_prob_m1 > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "# Assuming y_test is in binary format (0 or 1)\n",
    "y_val_true_m1 = y_test\n",
    "\n",
    "# Calculate precision and recall for binary classification\n",
    "precision_m1 = precision_score(y_val_true_m1, y_val_pred_m1)\n",
    "recall_m1 = recall_score(y_val_true_m1, y_val_pred_m1)\n",
    "\n",
    "# print the results\n",
    "print(f'Precision: {precision_m1:.4f}')\n",
    "print(f'Recall: {recall_m1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.7304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_m1 = f1_score(y_val_true_m1, y_val_pred_m1)\n",
    "print(f'F1-score: {f1_m1:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
